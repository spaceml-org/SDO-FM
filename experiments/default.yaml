# default.yaml

# general
experiment:
  name: "default"
  project: "sdofm"
  task: "" # options: pretrain_mae
  seed: 0
  disable_cuda: false
  disable_wandb: false
  wandb:
    entity: "fdlx"
    group: "sdofm-phase1"
    job_type: "pretrain"
    tags: []
    note : null
  fold: null
  evaluate: false   # skip training and only evaluate (requires checkpoint to be set)
  checkpoint: null  # this is the wandb run_id of the checkpoint to load
  device: null      # this is set automatically using the disable_cuda flag and torch.cuda.is_available()
  precision: 64     # 32, 64
  log_n_batches: 1000 # log every n training batches
  save_results: true # save full results to file and wandb
  distributed:
    enabled: false
    backend: "ddp"
    nproc_per_node: 1
    nnodes: 1
    node_rank: 0
    world_size: 2
    # node_rank: 0
    # local_rank: 0
    # master_addr: "localhost"
    # master_port: 12345

# dataset configuration
data:
  min_date: "2010-05-16 00:00:00"
  max_date: "2018-11-29 00:00:00"
  train_subsample: null
  val_test_subsample: null
  num_workers: 16    # set appropriately for your machine
  output_directory: "output"
  sdoml:
    base_directory: "base"
    instrument_sub_directory:
      hmi: "hmi"
      aia: "aia"
      eve: "eve"
    components: null # null for select all magnetic components ["Bx", "By", "Bz"]
    wavelengths: null # null for select all wavelengths channels ["131A","1600A","1700A","171A","193A","211A","304A","335A","94A"]
    ions: null: # null to select all ion channels ["C III", "Fe IX", "Fe VIII", "Fe X", "Fe XI", "Fe XII", "Fe XIII", "Fe XIV", "Fe XIX", "Fe XV", "Fe XVI", "Fe XVIII", "Fe XVI_2", "Fe XX", "Fe XX_2", "Fe XX_3", "H I", "H I_2", "H I_3", "He I", "He II", "He II_2", "He I_2", "Mg IX", "Mg X", "Mg X_2", "Ne VII", "Ne VIII", "O II", "O III", "O III_2", "O II_2", "O IV", "O IV_2", "O V", "O VI", "S XIV", "Si XII", "Si XII_2"]
    metadata: "meta"

# model configurations
model:
  # PRETRAINERS
  sdofm:
    encoder: "3d"
  prithvi:
    num_classes: 3
    freeze_encoder: false
    num_neck_filters: 32
    mae:
      img_size: 512
      patch_size: 16
      num_frames: 3
      tubelet_size: 1
      in_chans: 3
      embed_dim: 1024
      depth: 24
      num_heads: 16
      decoder_embed_dim: 512
      decoder_depth: 8
      decoder_num_heads: 16
      mlp_ratio: 4.0
      # norm_layer: defaults to nn.LayerNorm
      norm_pix_loss: False
    # decoder_depth: 8
    # decoder_embed_dim: 512
    # decoder_num_heads: 16
    # depth: 12
    # embed_dim: 768
    # img_size: 224
    # in_chans: 6
    # num_frames: 3
    # num_heads: 12
    # patch_size: 16
    # tubelet_size: 1
    # mask_ratio: 0.75
    # random_cropping: true
  # FINE-TUNERS
  dimming:
    dims: [128,64]
  # ML optimization arguments:
  opt:
    loss: "mse" # options: "mae", "mse", "mape"
    scheduler: "constant" #other options: "cosine", "plateau", "exp"
    scheduler_warmup: 0
    batch_size: 256
    learning_rate: 0.0001
    weight_decay: 0.0
    optimizer: "adam"
    epochs: 4
    patience: 2

# hydra configuration
hydra:
  mode: MULTIRUN
  run:
    dir: ${data.output_directory}/${now:%Y-%m-%d-%H-%M-%S}
  sweep:
    dir: ${hydra.run.dir}
    subdir: ${hydra.job.num}