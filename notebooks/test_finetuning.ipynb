{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/walsh/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import wandb\n",
    "from sdofm import utils\n",
    "from sdofm.datasets import SDOMLDataModule, DegradedSDOMLDataModule\n",
    "from sdofm.pretraining import MAE, SAMAE\n",
    "from sdofm.finetuning import Autocalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "\n",
    "cfg = omegaconf.OmegaConf.load(\"../experiments/pretrain_32.2M_samae_tpu.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sdofm.utils import flatten_dict\n",
    "# import yaml\n",
    "\n",
    "# data = flatten_dict(cfg, sep=\"___\")\n",
    "# with open('testingout.yaml', 'w+') as outfile:\n",
    "#     yaml.dump(data, outfile, default_flow_style=False)\n",
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from omegaconf import OmegaConf\n",
    "# OmegaConf.save(config, \"testout.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[* CACHE SYSTEM *] Found cached index data in /mnt/sdoml/cache/aligndata_AIA_FULL_12min.csv.\n",
      "[* CACHE SYSTEM *] Found cached normalization data in /mnt/sdoml/cache/normalizations_AIA_FULL_12min.json.\n",
      "[* CACHE SYSTEM *] Found cached HMI mask data in /mnt/sdoml/cache/hmi_mask_512x512.npy.\n"
     ]
    }
   ],
   "source": [
    "degraded_data_module = DegradedSDOMLDataModule(\n",
    "    hmi_path=None,\n",
    "    aia_path=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.aia\n",
    "    ),\n",
    "    eve_path=None,\n",
    "    components=cfg.data.sdoml.components,\n",
    "    wavelengths=cfg.data.sdoml.wavelengths,\n",
    "    ions=cfg.data.sdoml.ions,\n",
    "    frequency=cfg.data.sdoml.frequency,\n",
    "    batch_size=cfg.model.opt.batch_size,\n",
    "    num_workers=cfg.data.num_workers,\n",
    "    val_months=cfg.data.month_splits.val,\n",
    "    test_months=cfg.data.month_splits.test,\n",
    "    holdout_months=cfg.data.month_splits.holdout,\n",
    "    cache_dir=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.cache\n",
    "    ),\n",
    "    min_date=cfg.data.min_date,\n",
    "    max_date=cfg.data.max_date,\n",
    "    num_frames=1,\n",
    ")\n",
    "degraded_data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/walsh/repos/SDO-FM/output/2024-04-17-00-41-29/0/wandb/latest-run/files/config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39myaml\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m conf \u001b[39m=\u001b[39m yaml\u001b[39m.\u001b[39msafe_load(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     Path(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/home/walsh/repos/SDO-FM/output/2024-04-17-00-41-29/0/wandb/latest-run/files/config.yaml\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     )\u001b[39m.\u001b[39;49mread_text()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m backbone_cfg \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39munflatten_dict(conf, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m___\u001b[39m\u001b[39m\"\u001b[39m, wandb_mode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m backbone_cfg\n",
      "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1134\u001b[0m, in \u001b[0;36mPath.read_text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[39mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m encoding \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1134\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49mencoding, errors\u001b[39m=\u001b[39;49merrors) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m   1135\u001b[0m     \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1119\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1118\u001b[0m     encoding \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1119\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m, mode, buffering, encoding, errors,\n\u001b[1;32m   1120\u001b[0m                            newline)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/walsh/repos/SDO-FM/output/2024-04-17-00-41-29/0/wandb/latest-run/files/config.yaml'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "conf = yaml.safe_load(\n",
    "    Path(\n",
    "        \"/home/walsh/repos/SDO-FM/output/2024-04-17-00-41-29/0/wandb/latest-run/files/config.yaml\"\n",
    "    ).read_text()\n",
    ")\n",
    "backbone_cfg = utils.unflatten_dict(conf, sep=\"___\", wandb_mode=True)\n",
    "backbone_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 'mse',\n",
       " 'scheduler': 'constant',\n",
       " 'scheduler_warmup': 0,\n",
       " 'batch_size': 3,\n",
       " 'learning_rate': 0.0001,\n",
       " 'weight_decay': 0.0003,\n",
       " 'optimiser': 'adam',\n",
       " 'epochs': 4,\n",
       " 'patience': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone_cfg.model.opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::empty.memory_format' with arguments from the 'XLA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty.memory_format' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, QuantizedMeta, MkldnnCPU, SparseCPU, SparseCUDA, SparseMeta, SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31419 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44504 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:951 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nQuantizedMeta: registered at aten/src/ATen/RegisterQuantizedMeta.cpp:105 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:515 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseMeta: registered at aten/src/ATen/RegisterSparseMeta.cpp:249 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nSparseCsrMeta: registered at aten/src/ATen/RegisterSparseCsrMeta.cpp:185 [kernel]\nBackendSelect: registered at aten/src/ATen/RegisterBackendSelect.cpp:807 [kernel]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: fallthrough registered at ../aten/src/ATen/ZeroTensorFallback.cpp:90 [kernel]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:17415 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m backbone_cfg\u001b[39m=\u001b[39mcfg\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m backbone \u001b[39m=\u001b[39m SAMAE\u001b[39m.\u001b[39;49mload_from_checkpoint(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbackbone_cfg\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mmae,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbackbone_cfg\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msamae,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     optimiser\u001b[39m=\u001b[39;49mbackbone_cfg\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mopt\u001b[39m.\u001b[39;49moptimiser,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     lr\u001b[39m=\u001b[39;49mbackbone_cfg\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mopt\u001b[39m.\u001b[39;49mlearning_rate,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mbackbone_cfg\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mopt\u001b[39m.\u001b[39;49mweight_decay,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     checkpoint_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/home/walsh/SDO-FM/outputs/2024-05-23/22-51-44/lightning_logs/version_0/checkpoints/epoch=2-step=132.ckpt\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-tpu/home/walsh/SDO-FM/notebooks/test_finetuning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[0;32m~/pytorch-lightning/src/lightning/pytorch/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m instance \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scripting:\n\u001b[1;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe classmethod `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` cannot be called on an instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmethod(\u001b[39mcls\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/pytorch-lightning/src/lightning/pytorch/core/module.py:1585\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1498\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1503\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1504\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:\n\u001b[1;32m   1505\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m \u001b[39m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1583\u001b[0m \n\u001b[1;32m   1584\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1585\u001b[0m     loaded \u001b[39m=\u001b[39m _load_from_checkpoint(\n\u001b[1;32m   1586\u001b[0m         \u001b[39mcls\u001b[39;49m,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1587\u001b[0m         checkpoint_path,\n\u001b[1;32m   1588\u001b[0m         map_location,\n\u001b[1;32m   1589\u001b[0m         hparams_file,\n\u001b[1;32m   1590\u001b[0m         strict,\n\u001b[1;32m   1591\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1592\u001b[0m     )\n\u001b[1;32m   1593\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/pytorch-lightning/src/lightning/pytorch/core/saving.py:63\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m map_location \u001b[39m=\u001b[39m map_location \u001b[39mor\u001b[39;00m _default_map_location\n\u001b[1;32m     62\u001b[0m \u001b[39mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 63\u001b[0m     checkpoint \u001b[39m=\u001b[39m pl_load(checkpoint_path, map_location\u001b[39m=\u001b[39;49mmap_location)\n\u001b[1;32m     65\u001b[0m \u001b[39m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m     66\u001b[0m checkpoint \u001b[39m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m     67\u001b[0m     checkpoint, checkpoint_path\u001b[39m=\u001b[39m(checkpoint_path \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(checkpoint_path, (\u001b[39mstr\u001b[39m, Path)) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m )\n",
      "File \u001b[0;32m~/pytorch-lightning/src/lightning/fabric/utilities/cloud_io.py:57\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     55\u001b[0m fs \u001b[39m=\u001b[39m get_filesystem(path_or_url)\n\u001b[1;32m     56\u001b[0m \u001b[39mwith\u001b[39;00m fs\u001b[39m.\u001b[39mopen(path_or_url, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(f, map_location\u001b[39m=\u001b[39;49mmap_location)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1024\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[1;32m   1026\u001b[0m                      map_location,\n\u001b[1;32m   1027\u001b[0m                      pickle_module,\n\u001b[1;32m   1028\u001b[0m                      overall_storage\u001b[39m=\u001b[39;49moverall_storage,\n\u001b[1;32m   1029\u001b[0m                      \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m   1030\u001b[0m \u001b[39mif\u001b[39;00m mmap:\n\u001b[1;32m   1031\u001b[0m     f_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(f, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1445\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1446\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1448\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1449\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1450\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtorch.load.metadata\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mserialization_id\u001b[39m\u001b[39m\"\u001b[39m: zip_file\u001b[39m.\u001b[39mserialization_id()}\n\u001b[1;32m   1451\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1213\u001b[0m         dispatch[key[\u001b[39m0\u001b[39;49m]](\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   1214\u001b[0m \u001b[39mexcept\u001b[39;00m _Stop \u001b[39mas\u001b[39;00m stopinst:\n\u001b[1;32m   1215\u001b[0m     \u001b[39mreturn\u001b[39;00m stopinst\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:1590\u001b[0m, in \u001b[0;36m_Unpickler.load_reduce\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1588\u001b[0m args \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39mpop()\n\u001b[1;32m   1589\u001b[0m func \u001b[39m=\u001b[39m stack[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m-> 1590\u001b[0m stack[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:334\u001b[0m, in \u001b[0;36m_rebuild_device_tensor_from_numpy\u001b[0;34m(data, dtype, device, requires_grad)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_rebuild_device_tensor_from_numpy\u001b[39m(data, dtype, device, requires_grad):\n\u001b[0;32m--> 334\u001b[0m     tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(data)\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    335\u001b[0m     tensor\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m requires_grad\n\u001b[1;32m    336\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::empty.memory_format' with arguments from the 'XLA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty.memory_format' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, QuantizedMeta, MkldnnCPU, SparseCPU, SparseCUDA, SparseMeta, SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31419 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44504 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:951 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nQuantizedMeta: registered at aten/src/ATen/RegisterQuantizedMeta.cpp:105 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:515 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1573 [kernel]\nSparseMeta: registered at aten/src/ATen/RegisterSparseMeta.cpp:249 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:1276 [kernel]\nSparseCsrMeta: registered at aten/src/ATen/RegisterSparseCsrMeta.cpp:185 [kernel]\nBackendSelect: registered at aten/src/ATen/RegisterBackendSelect.cpp:807 [kernel]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: fallthrough registered at ../aten/src/ATen/ZeroTensorFallback.cpp:90 [kernel]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19078 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:17415 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "backbone_cfg=cfg\n",
    "backbone = SAMAE.load_from_checkpoint(\n",
    "    **backbone_cfg.model.mae,\n",
    "    **backbone_cfg.model.samae,\n",
    "    optimiser=backbone_cfg.model.opt.optimiser,\n",
    "    lr=backbone_cfg.model.opt.learning_rate,\n",
    "    weight_decay=backbone_cfg.model.opt.weight_decay,\n",
    "    checkpoint_path=\"/home/walsh/SDO-FM/outputs/2024-05-23/22-51-44/lightning_logs/version_0/checkpoints/epoch=2-step=132.ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autocalibration(\n",
    "    **cfg.model.mae,\n",
    "    **cfg.model.dimming,\n",
    "    optimiser=cfg.model.opt.optimiser,\n",
    "    lr=cfg.model.opt.learning_rate,\n",
    "    weight_decay=cfg.model.opt.weight_decay,\n",
    "    backbone=backbone,\n",
    "    hyperparam_ignore=['backbone']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type                                 | Params\n",
      "-----------------------------------------------------------------------\n",
      "0 | backbone      | SAMAE                                | 32.2 M\n",
      "1 | encoder       | PrithviEncoder                       | 32.2 M\n",
      "2 | decoder       | ConvTransformerTokensToEmbeddingNeck | 28.9 K\n",
      "3 | head          | Autocalibration13                    | 93.4 K\n",
      "4 | loss_function | MSELoss                              | 0     \n",
      "-----------------------------------------------------------------------\n",
      "122 K     Trainable params\n",
      "32.2 M    Non-trainable params\n",
      "32.3 M    Total params\n",
      "129.203   Total estimated model params size (MB)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/walsh/repos/SDO-FM/notebooks/test_finetuning.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-workbench-n1-16cpu-60ram-t4x2.us-central1-a.sdo-fm-2024/home/walsh/repos/SDO-FM/notebooks/test_finetuning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-workbench-n1-16cpu-60ram-t4x2.us-central1-a.sdo-fm-2024/home/walsh/repos/SDO-FM/notebooks/test_finetuning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, accelerator\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mexperiment\u001b[39m.\u001b[39maccelerator, max_epochs\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mepochs\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-workbench-n1-16cpu-60ram-t4x2.us-central1-a.sdo-fm-2024/home/walsh/repos/SDO-FM/notebooks/test_finetuning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsdofm-workbench-n1-16cpu-60ram-t4x2.us-central1-a.sdo-fm-2024/home/walsh/repos/SDO-FM/notebooks/test_finetuning.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model\u001b[39m=\u001b[39;49mmodel, datamodule\u001b[39m=\u001b[39;49mdegraded_data_module)\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:970\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    967\u001b[0m     call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_fit_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    968\u001b[0m     call\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_fit_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 970\u001b[0m _log_hyperparams(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mrestore_checkpoint_after_setup:\n\u001b[1;32m    973\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: restoring module and callbacks from checkpoint path: \u001b[39m\u001b[39m{\u001b[39;00mckpt_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/lightning/pytorch/loggers/utilities.py:95\u001b[0m, in \u001b[0;36m_log_hyperparams\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     93\u001b[0m     logger\u001b[39m.\u001b[39mlog_hyperparams(hparams_initial)\n\u001b[1;32m     94\u001b[0m logger\u001b[39m.\u001b[39mlog_graph(pl_module)\n\u001b[0;32m---> 95\u001b[0m logger\u001b[39m.\u001b[39;49msave()\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/lightning_utilities/core/rank_zero.py:42\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe `rank_zero_only.rank` needs to be set before use\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m rank \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m default\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/lightning/pytorch/loggers/tensorboard.py:221\u001b[0m, in \u001b[0;36mTensorBoardLogger.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39m# save the metatags file if it doesn't exist and the log directory exists\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39mif\u001b[39;00m _is_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs, dir_path) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs\u001b[39m.\u001b[39misfile(hparams_file):\n\u001b[0;32m--> 221\u001b[0m     save_hparams_to_yaml(hparams_file, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhparams)\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:354\u001b[0m, in \u001b[0;36msave_hparams_to_yaml\u001b[0;34m(config_yaml, hparams, use_omegaconf)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m     v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mname \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, Enum) \u001b[39melse\u001b[39;00m v\n\u001b[0;32m--> 354\u001b[0m     yaml\u001b[39m.\u001b[39;49mdump(v)\n\u001b[1;32m    355\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     warn(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSkipping \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m parameter because it is not possible to safely dump to YAML.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/yaml/__init__.py:253\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(data, stream, Dumper, **kwds)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(data, stream\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, Dumper\u001b[39m=\u001b[39mDumper, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m    249\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m    Serialize a Python object into a YAML stream.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m    If stream is None, return the produced string instead.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mreturn\u001b[39;00m dump_all([data], stream, Dumper\u001b[39m=\u001b[39;49mDumper, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/yaml/__init__.py:241\u001b[0m, in \u001b[0;36mdump_all\u001b[0;34m(documents, stream, Dumper, default_style, default_flow_style, canonical, indent, width, allow_unicode, line_break, encoding, explicit_start, explicit_end, version, tags, sort_keys)\u001b[0m\n\u001b[1;32m    239\u001b[0m     dumper\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    240\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m documents:\n\u001b[0;32m--> 241\u001b[0m         dumper\u001b[39m.\u001b[39;49mrepresent(data)\n\u001b[1;32m    242\u001b[0m     dumper\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    243\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/yaml/representer.py:27\u001b[0m, in \u001b[0;36mBaseRepresenter.represent\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepresent\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m---> 27\u001b[0m     node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepresent_data(data)\n\u001b[1;32m     28\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mserialize(node)\n\u001b[1;32m     29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresented_objects \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/yaml/representer.py:52\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m data_type \u001b[39min\u001b[39;00m data_types:\n\u001b[1;32m     51\u001b[0m     \u001b[39mif\u001b[39;00m data_type \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39myaml_multi_representers:\n\u001b[0;32m---> 52\u001b[0m         node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49myaml_multi_representers[data_type](\u001b[39mself\u001b[39;49m, data)\n\u001b[1;32m     53\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/yaml/representer.py:342\u001b[0m, in \u001b[0;36mRepresenter.represent_object\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    339\u001b[0m function_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (function\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m, function\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m listitems \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m dictitems \\\n\u001b[1;32m    341\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(state, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m newobj:\n\u001b[0;32m--> 342\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepresent_mapping(\n\u001b[1;32m    343\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mtag:yaml.org,2002:python/object:\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mfunction_name, state)\n\u001b[1;32m    344\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m listitems \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m dictitems  \\\n\u001b[1;32m    345\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(state, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m state:\n\u001b[1;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresent_sequence(tag\u001b[39m+\u001b[39mfunction_name, args)\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/yaml/representer.py:118\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_mapping\u001b[0;34m(self, tag, mapping, flow_style)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mfor\u001b[39;00m item_key, item_value \u001b[39min\u001b[39;00m mapping:\n\u001b[1;32m    117\u001b[0m     node_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresent_data(item_key)\n\u001b[0;32m--> 118\u001b[0m     node_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepresent_data(item_value)\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(node_key, ScalarNode) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m node_key\u001b[39m.\u001b[39mstyle):\n\u001b[1;32m    120\u001b[0m         best_style \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/yaml/representer.py:52\u001b[0m, in \u001b[0;36mBaseRepresenter.represent_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m data_type \u001b[39min\u001b[39;00m data_types:\n\u001b[1;32m     51\u001b[0m     \u001b[39mif\u001b[39;00m data_type \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39myaml_multi_representers:\n\u001b[0;32m---> 52\u001b[0m         node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49myaml_multi_representers[data_type](\u001b[39mself\u001b[39;49m, data)\n\u001b[1;32m     53\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/yaml/representer.py:330\u001b[0m, in \u001b[0;36mRepresenter.represent_object\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    328\u001b[0m     listitems \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(listitems)\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m dictitems \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m     dictitems \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39;49m(dictitems)\n\u001b[1;32m    331\u001b[0m \u001b[39mif\u001b[39;00m function\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__newobj__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    332\u001b[0m     function \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=1, accelerator=cfg.experiment.accelerator, max_epochs=cfg.model.opt.epochs\n",
    ")\n",
    "trainer.fit(model=model, datamodule=degraded_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdofm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
