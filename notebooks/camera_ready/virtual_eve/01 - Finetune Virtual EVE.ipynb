{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual EVE From Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background:\n",
    "This notebook provides an example of finetuning with SDOFM. In this case we create a virtual eve instrument, starting the training from a SDOFM pretrained foundation model, accomplishing a production ready model much faster than training from scratch.\n",
    "\n",
    "#### Foundation Models\n",
    "The process is akin to that of transfer learning, a method typically used in computer vision, for example by freezing a feature extracting neural network pretrained on imagenet. For an extensive treatment of the method of transfer learning, as it was considered before the advent of large modern models, please see [review paper](https://arxiv.org/abs/1811.08883).\n",
    "\n",
    "For the sake of conceptual understanding we can think of the foundation model as a feature extractor depicted in the figure below as the \"Head\". \n",
    "\n",
    "This head model is pretrained on a large dataset, in this case SDO, and the motivation here is that the foundation head model has a built-in understanding of the dataset it was pretrained on. \n",
    "\n",
    "During the pretraining step, the foundation model is unfrozen, and the entire model is fine-tuned on the foundational dataset. This process is called transfer learning, and it is used to train models on smaller datasets, where training from scratch would not be feasible. This is our approach in this notebook.\n",
    "\n",
    "![Figure 1: Architectural Diagram](assets/architecture_diag.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "For this section, please be sure to be located in the project root directory before executing any commands. None of the cells in this section are meant to be ran from the notebook IDE, but rather your terminal.\n",
    "\n",
    "#### System Requirements\n",
    "This tutorial assumes that you have conda or miniconda installed and are on a linux or macos machine. It's advisable to install miniconda if you have to decide between the two (smaller install), however, if you already have conda installed, you can skip on to the next step.\n",
    "\n",
    "For instructions on installing miniconda, please see [Miniconda Installation](https://docs.anaconda.com/miniconda/miniconda-install/).\n",
    "\n",
    "##### Python environment setup\n",
    "After you're sure you have conda installed on your system, please run the following command from the project root directory to install a new conda environment\n",
    "\n",
    "`conda env create -f notebooks/camera_ready/virtual_eve/conda_env.yml` .\n",
    "\n",
    "And activate the newly created environment with:\n",
    "\n",
    "`conda activate virtual-eve-finetuning`.\n",
    "\n",
    "Next, install the required python libraries:\n",
    "\n",
    "`pip install -r notebooks/camera_ready/virtual_eve/requirements.txt`\n",
    "\n",
    "And the local sdofm package:\n",
    "\n",
    "`pip install -e .`\n",
    "\n",
    "Lastly, make sure to select the correct python kernel associated with this environment, (likely located in `${CONDA_PREFIX_1}/envs/virtual-eve-finetuning/bin/python`)\n",
    "\n",
    "Nice, you should now be all set to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the finetuning\n",
    "\n",
    "We begin by importing the newly installed libraries we will need tp run this notebook. Note: the import cell below is the first one you should be executing in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: SunpyUserWarning: Importing sunpy.map without its extra dependencies may result in errors.\n",
      "The following packages are not installed:\n",
      "['mpl-animators>=1.0.0', 'reproject>=0.9.0']\n",
      "To install sunpy with these dependencies use `pip install sunpy[map]` or `pip install sunpy[all]` for all extras. \n",
      "If you installed sunpy via conda, please report this to the community channel: https://matrix.to/#/#sunpy:openastronomy.org [sunpy.util.sysinfo]\n",
      "WARNING: SunpyUserWarning: Importing sunpy.visualization without its extra dependencies may result in errors.\n",
      "The following packages are not installed:\n",
      "['mpl-animators>=1.0.0']\n",
      "To install sunpy with these dependencies use `pip install sunpy[visualization]` or `pip install sunpy[all]` for all extras. \n",
      "If you installed sunpy via conda, please report this to the community channel: https://matrix.to/#/#sunpy:openastronomy.org [sunpy.util.sysinfo]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import omegaconf\n",
    "from sdofm.datasets import SDOMLDataModule\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the appropriate configuration file for this run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = omegaconf.OmegaConf.load(\"finetune_virtualeve_config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration file specifies various parameters for the model run such as \n",
    "\n",
    "- Experiment Config: Where to find the opensource pretrained model weights\n",
    "- Data configuration: where to load the input data from, data metadata, etc\n",
    "- Run parameters: Number of epochs, etc\n",
    "- Misc: log levels, etc.\n",
    "\n",
    "We can interrogate the values in the configuration file by either opening the configuration file itself, or printing the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_level': 'DEBUG',\n",
       " 'experiment': {'name': None, 'project': 'sdofm', 'task': 'finetune', 'model': 'virtualeve', 'resuming': False, 'checkpoint': None, 'backbone': {'checkpoint': 'assets/model.ckpt', 'model': 'mae'}, 'seed': 0, 'disable_cuda': False, 'wandb': {'enable': False, 'entity': '', 'group': '', 'job_type': 'finetune', 'tags': [], 'notes': '', 'output_directory': 'wandb_output', 'log_model': 'all'}, 'gcp_storage': {'enabled': True, 'bucket': 'sdofm-checkpoints'}, 'fold': None, 'evaluate': False, 'device': None, 'precision': 'bf16-true', 'log_n_batches': 1000, 'save_results': True, 'accelerator': 'auto', 'profiler': None, 'distributed': {'enabled': True, 'world_size': 'auto', 'strategy': 'ddp_find_unused_parameters_true'}, 'log_every_n_steps': 5},\n",
       " 'data': {'min_date': '2011-01-01 00:00:00.00', 'max_date': '2011-12-31 23:59:59.99', 'month_splits': {'val': [11], 'test': [12], 'holdout': []}, 'num_workers': 4, 'prefetch_factor': 3, 'num_frames': 1, 'drop_frame_dim': True, 'sdoml': {'base_directory': '/mnt/sdoml', 'sub_directory': {'hmi': 'HMI.zarr', 'aia': 'AIA.zarr', 'eve': 'EVE_legacy.zarr', 'cache': 'cache'}, 'components': None, 'wavelengths': None, 'ions': None, 'frequency': '12min', 'mask_with_hmi_threshold': None, 'feature_engineering': {'enabled': False}}},\n",
       " 'model': {'mae': {'img_size': 512, 'patch_size': 16, 'num_frames': 1, 'tubelet_size': 1, 'in_chans': 9, 'embed_dim': 512, 'depth': 24, 'num_heads': 16, 'decoder_embed_dim': 512, 'decoder_depth': 8, 'decoder_num_heads': 16, 'mlp_ratio': 4.0, 'norm_layer': 'LayerNorm', 'norm_pix_loss': False, 'masking_ratio': 0.5}, 'samae': {'masking_type': 'random', 'active_region_mu_degs': 15.73, 'active_region_std_degs': 6.14, 'active_region_scale': 1.0, 'active_region_abs_lon_max_degs': 60, 'active_region_abs_lat_max_degs': 60}, 'nvae': {'use_se': True, 'res_dist': True, 'num_x_bits': 8, 'num_latent_scales': 3, 'num_groups_per_scale': 1, 'num_latent_per_group': 1, 'ada_groups': True, 'min_groups_per_scale': 1, 'num_channels_enc': 30, 'num_channels_dec': 30, 'num_preprocess_blocks': 2, 'num_preprocess_cells': 2, 'num_cell_per_cond_enc': 2, 'num_postprocess_blocks': 2, 'num_postprocess_cells': 2, 'num_cell_per_cond_dec': 2, 'num_mixture_dec': 1, 'num_nf': 2, 'kl_anneal_portion': 0.3, 'kl_const_portion': 0.0001, 'kl_const_coeff': 0.0001, 'weight_decay_norm_anneal': True, 'weight_decay_norm_init': 1.0, 'weight_decay_norm': 0.01}, 'autocalibration': {'freeze_encoder': True, 'num_neck_filters': 32, 'output_dim': 1, 'loss': 'mse'}, 'virtualeve': {'freeze_encoder': True, 'num_neck_filters': 32, 'cnn_model': 'efficientnet_b3', 'lr_linear': 0.01, 'lr_cnn': 0.0001, 'cnn_dp': 0.75, 'epochs_linear': 20}, 'opt': {'loss': 'mse', 'scheduler': 'constant', 'scheduler_warmup': 0, 'batch_size': 1, 'learning_rate': 0.0001, 'weight_decay': 0.0003, 'optimiser': 'adam', 'epochs': 50, 'patience': 2}},\n",
       " 'hydra': {'mode': 'RUN'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the SDOMLDataModule, which defines how we interact with the training dataset for the finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[* CACHE SYSTEM *] Found cached index data in /mnt/sdoml/cache/aligndata_HMI_FULL_AIA_FULL_EVE_FULL_12min.csv.\n",
      "[* CACHE SYSTEM *] Found cached normalization data in /mnt/sdoml/cache/normalizations_HMI_FULL_AIA_FULL_EVE_FULL_12min.json.\n",
      "[* CACHE SYSTEM *] Found cached HMI mask data in /mnt/sdoml/cache/hmi_mask_512x512.npy.\n"
     ]
    }
   ],
   "source": [
    "data_module = SDOMLDataModule(\n",
    "     hmi_path=(\n",
    "         os.path.join(\n",
    "            cfg.data.sdoml.base_directory,\n",
    "            cfg.data.sdoml.sub_directory.hmi,\n",
    "        )\n",
    "        if cfg.data.sdoml.sub_directory.hmi\n",
    "        else None\n",
    "    ),\n",
    "    aia_path=(\n",
    "        os.path.join(\n",
    "            cfg.data.sdoml.base_directory,\n",
    "            cfg.data.sdoml.sub_directory.aia,\n",
    "        )\n",
    "        if cfg.data.sdoml.sub_directory.aia\n",
    "        else None\n",
    "    ),\n",
    "    eve_path=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.eve\n",
    "    ),\n",
    "    components=cfg.data.sdoml.components,\n",
    "    wavelengths=cfg.data.sdoml.wavelengths,\n",
    "    ions=cfg.data.sdoml.ions,\n",
    "    frequency=cfg.data.sdoml.frequency,\n",
    "    batch_size=cfg.model.opt.batch_size,\n",
    "    num_workers=cfg.data.num_workers,\n",
    "    val_months=cfg.data.month_splits.val,\n",
    "    test_months=cfg.data.month_splits.test,\n",
    "    holdout_months=cfg.data.month_splits.holdout,\n",
    "    cache_dir=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.cache\n",
    "    ),\n",
    "    min_date=cfg.data.min_date,\n",
    "    max_date=cfg.data.max_date,\n",
    "    num_frames=cfg.data.num_frames,\n",
    "    drop_frame_dim=cfg.data.drop_frame_dim,\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(data_module.train_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a finetuning model\n",
    "Now time to get serious, this model will become our model's \"head.\" The objective of this component is to take now a set of finetuned embeddings and have them predict our true science task. This model was created during FDL-X 2023 and is used as an quick example. It has a switching mode that transitions the model from linear to influenced by a CNN after a defned number of epochs. We're going to do this with Pytorch Lighning for keep hardware agnostic. \n",
    "\n",
    "We first import necessary components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from sdofm import BaseModule\n",
    "import sys\n",
    "from sdofm.models.virtualeve import unnormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearIrradianceModel(BaseModule):\n",
    "    def __init__(self, d_input, d_output, eve_norm):\n",
    "        super().__init__()\n",
    "        # self.eve_norm = eve_norm\n",
    "        self.n_channels = d_input\n",
    "        self.outSize = d_output\n",
    "\n",
    "        self.model = nn.Linear(2 * self.n_channels, self.outSize)\n",
    "        self.loss_func = nn.HuberLoss()  # consider MSE\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_irradiance = torch.mean(x, dim=(2, 3))\n",
    "        std_irradiance = torch.std(x, dim=(2, 3))\n",
    "        x = self.model(torch.cat((mean_irradiance, std_irradiance), dim=1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the CNN efficientnet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNIrradianceModel(BaseModule):\n",
    "    def __init__(self, d_input, d_output, eve_norm, dp=0.75):\n",
    "        super().__init__()\n",
    "\n",
    "        model = torchvision.models.efficientnet_b3(weights=\"IMAGENET1K_V1\")\n",
    "        conv1_out = model.features[0][0].out_channels\n",
    "        model.features[0][0] = nn.Conv2d(\n",
    "            d_input,\n",
    "            conv1_out,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(2, 2),\n",
    "            padding=(1, 1),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        lin_in = model.classifier[1].in_features\n",
    "        classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dp, inplace=True),\n",
    "            nn.Linear(in_features=lin_in, out_features=d_output, bias=True),\n",
    "        )\n",
    "        model.classifier = classifier\n",
    "\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__.startswith(\"Dropout\"):\n",
    "                m.p = dp\n",
    "\n",
    "        self.model = model\n",
    "        self.loss_func = nn.HuberLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the hybrid switching model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridIrradianceModel(BaseModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input,\n",
    "        d_output,\n",
    "        eve_norm,\n",
    "        *args,\n",
    "        lr_linear=0.01,\n",
    "        lr_cnn=0.0001,\n",
    "        cnn_dp=0.75,\n",
    "        ln_params=None,  # used in lambda function out of scope?\n",
    "        epochs_linear=None,  # out of scope\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # self.eve_norm = torch.Tensor(self.eve_norm).float() #eve_norm\n",
    "        self.register_buffer(\"eve_norm\", torch.Tensor(eve_norm).float())\n",
    "        self.n_channels = d_input\n",
    "        self.outSize = d_output\n",
    "        # self.ln_params = ln_params # unused\n",
    "        self.lr_linear = lr_linear\n",
    "        self.lr_cnn = lr_cnn\n",
    "        self.train_mode = \"linear\"\n",
    "\n",
    "        self.ln_model = LinearIrradianceModel(d_input, d_output, eve_norm)\n",
    "        self.cnn_model = CNNIrradianceModel(\n",
    "            d_input, d_output, eve_norm, dp=cnn_dp\n",
    "        )\n",
    "        self.loss_func = nn.HuberLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ln_model.forward(x) + self.cnn_lambda * self.cnn_model.forward(x)\n",
    "\n",
    "    def forward_unnormalize(self, x):\n",
    "        return self.unnormalize(self.forward(x))\n",
    "\n",
    "    def unnormalize(self, y):\n",
    "        # eve_norm = torch.tensor(self.eve_norm).float()\n",
    "        norm_mean = self.eve_norm[0]\n",
    "        norm_stdev = self.eve_norm[1]\n",
    "        y = y * norm_stdev[None] + norm_mean[None]  # .to(y) .to(y)\n",
    "        return y\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.loss_func(y_pred, y)\n",
    "\n",
    "        # print(\"t: trying un\")\n",
    "        y = self.unnormalize(y)\n",
    "        y_pred = self.unnormalize(y_pred)\n",
    "        # print(\"t: success un\")\n",
    "\n",
    "        epsilon = sys.float_info.epsilon\n",
    "        # computing relative absolute error\n",
    "        rae = torch.abs((y - y_pred) / (torch.abs(y) + epsilon)) * 100\n",
    "        av_rae = rae.mean()\n",
    "\n",
    "        self.log_everything(\"train\", loss, av_rae, float(self.cnn_lambda))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.loss_func(y_pred, y)\n",
    "\n",
    "        y = self.unnormalize(y)\n",
    "        y_pred = self.unnormalize(y_pred)\n",
    "\n",
    "        epsilon = sys.float_info.epsilon\n",
    "        # computing relative absolute error\n",
    "        rae = torch.abs((y - y_pred) / (torch.abs(y) + epsilon)) * 100\n",
    "        av_rae = rae.mean()\n",
    "        av_rae_wl = rae.mean(0)\n",
    "        # compute average cross-correlation\n",
    "        cc = torch.tensor(\n",
    "            [\n",
    "                torch.corrcoef(torch.stack([y[i], y_pred[i]]))[0, 1]\n",
    "                for i in range(y.shape[0])\n",
    "            ]\n",
    "        ).mean()\n",
    "        # compute mean absolute error\n",
    "        mae = torch.abs(y - y_pred).mean()\n",
    "\n",
    "        self.log_everything(\n",
    "            \"val\", loss, av_rae, float(self.cnn_lambda), av_rae_wl, mae, cc\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.loss_func(y_pred, y)\n",
    "\n",
    "        y = self.unnormalize(y)\n",
    "        y_pred = self.unnormalize(y_pred)\n",
    "\n",
    "        epsilon = sys.float_info.epsilon\n",
    "        rae = torch.abs((y - y_pred) / (torch.abs(y) + epsilon)) * 100\n",
    "        av_rae = rae.mean()\n",
    "        av_rae_wl = rae.mean(0)\n",
    "        # compute average cross-correlation\n",
    "        cc = torch.tensor(\n",
    "            [\n",
    "                torch.corrcoef(torch.stack([y[i], y_pred[i]]))[0, 1]\n",
    "                for i in range(y.shape[0])\n",
    "            ]\n",
    "        ).mean()\n",
    "        # mean absolute error\n",
    "        mae = torch.abs(y - y_pred).mean()\n",
    "\n",
    "        self.log_everything(\n",
    "            \"test\", loss, av_rae, float(self.cnn_lambda), av_rae_wl, mae, cc\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def log_everything(\n",
    "        self, mode, loss, av_rae, train_lambda_cnn, av_rae_wl=None, mae=None, cc=None\n",
    "    ):\n",
    "        # the .to() calls are a fix for https://github.com/Lightning-AI/pytorch-lightning/issues/18803\n",
    "        self.log(\n",
    "            f\"{mode}_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            f\"{mode}_RAE\",\n",
    "            av_rae,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": self.ln_model.parameters()},\n",
    "                {\"params\": self.cnn_model.parameters(), \"lr\": self.lr_cnn},\n",
    "            ],\n",
    "            lr=self.lr_linear,\n",
    "        )\n",
    "\n",
    "    def set_train_mode(self, mode):\n",
    "        if mode == \"linear\":\n",
    "            self.train_mode = \"linear\"\n",
    "            self.cnn_lambda = 0.0\n",
    "            self.cnn_model.freeze()\n",
    "            self.ln_model.unfreeze()\n",
    "        elif mode == \"cnn\":\n",
    "            self.train_mode = \"cnn\"\n",
    "            self.cnn_lambda = 0.01\n",
    "            self.cnn_model.unfreeze()\n",
    "            self.ln_model.freeze()\n",
    "        else:\n",
    "            raise NotImplemented(f\"Mode not supported: {mode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the backbone model. In this example we're loading a Masked Autoencoder and an NVAE for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using <class 'sdofm.datasets.SDOML.SDOMLDataModule'> Data Class\n",
      "[* CACHE SYSTEM *] Found cached index data in /mnt/sdoml/cache/aligndata_AIA_FULL_12min.csv.\n",
      "[* CACHE SYSTEM *] Found cached normalization data in /mnt/sdoml/cache/normalizations_AIA_FULL_12min.json.\n",
      "[* CACHE SYSTEM *] Found cached HMI mask data in /mnt/sdoml/cache/hmi_mask_512x512.npy.\n",
      "Loading checkpoint...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from pretrain import Pretrainer\n",
    "MAE = Pretrainer(cfg, logger=None, is_backbone=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the full model\n",
    "Now that we have a suitable head model completing our objective, predicting EVE from the AIA instrument we can construct our full model. This is broken down by the sections as defined in the figure above. The backbone is set into finetuning mode and the encoder is explictly frozen. These outputs are fed into a decoding layer before they are ready for non-transformer models, such as our example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = MAE.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdofm.models import WrapEncoder, ConvTransformerTokensToEmbeddingNeck\n",
    "\n",
    "class VirtualEVEModel(BaseModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            # Backbone parameters\n",
    "            img_size: int = 512,\n",
    "            patch_size: int = 16,\n",
    "            embed_dim: int = 128,\n",
    "            num_frames: int = 1,\n",
    "            # Neck parameters\n",
    "            num_neck_filters: int = 32,\n",
    "            # Head parameters\n",
    "            # d_input=None,\n",
    "            cnn_model: str = \"efficientnet_b3\",\n",
    "            lr_linear: float = 0.01,\n",
    "            lr_cnn: float = 0.0001,\n",
    "            cnn_dp: float = 0.75,\n",
    "            epochs_linear: int = 50,\n",
    "            d_output=None,\n",
    "            eve_norm=None,\n",
    "            # for finetuning\n",
    "            backbone: object = None,\n",
    "            freeze_encoder: bool = True,\n",
    "            # all else\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        ):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.eve_norm = eve_norm\n",
    "\n",
    "            self.backbone = backbone\n",
    "            self.encoder = WrapEncoder(self.backbone)\n",
    "\n",
    "            if freeze_encoder:\n",
    "                self.encoder.eval()\n",
    "                for param in self.encoder.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            num_tokens = img_size // patch_size\n",
    "\n",
    "            # NECK\n",
    "            self.decoder = ConvTransformerTokensToEmbeddingNeck(\n",
    "                embed_dim=embed_dim,\n",
    "                output_embed_dim=num_neck_filters,\n",
    "                Hp=num_tokens,\n",
    "                Wp=num_tokens,\n",
    "                drop_cls_token=True,\n",
    "                num_frames=num_frames,\n",
    "            )\n",
    "\n",
    "            # HEAD\n",
    "            self.head = HybridIrradianceModel(\n",
    "                # virtual eve\n",
    "                d_input=num_neck_filters,\n",
    "                d_output=d_output,\n",
    "                eve_norm=eve_norm,\n",
    "                # from config\n",
    "                # cnn_model=cnn_model,\n",
    "                lr_linear=lr_linear,\n",
    "                lr_cnn=lr_cnn,\n",
    "                cnn_dp=cnn_dp,\n",
    "                # epochs_linear=epochs_linear,\n",
    "            )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, eve = batch\n",
    "        x = self.encoder(imgs[:, :9, :, :])\n",
    "        y_hat = self.head(self.decoder(x))\n",
    "        loss = self.head.loss_func(y_hat, eve[:, :38, 0])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, eve = batch\n",
    "        x = self.encoder(imgs[:, :9, :, :])\n",
    "        y_hat = self.head(self.decoder(x))\n",
    "        # print(eve.shape)\n",
    "        loss = self.head.loss_func(y_hat, eve[:, :38, 0])\n",
    "        self.log(\"val_loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then initialised with the params as defined in the experiment configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'backbone' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['backbone'])`.\n"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "\n",
    "backbone_params = {}\n",
    "backbone_params[\"img_size\"] = cfg.model.mae.img_size\n",
    "backbone_params[\"patch_size\"] = cfg.model.mae.patch_size\n",
    "backbone_params[\"embed_dim\"] = cfg.model.mae.embed_dim\n",
    "backbone_params[\"num_frames\"] = cfg.model.mae.num_frames\n",
    "\n",
    "model = VirtualEVEModel(\n",
    "    # backbone\n",
    "    **backbone_params,\n",
    "    # virtual eve params\n",
    "    **cfg.model.virtualeve,\n",
    "    d_output=len(data_module.ions),\n",
    "    eve_norm=np.array(\n",
    "        data_module.normalizations[\"EVE\"][\"eve_norm\"],\n",
    "        dtype=np.float32,\n",
    "    ),\n",
    "    # general\n",
    "    # optimiser=cfg.model.opt.optimiser,\n",
    "    # lr=cfg.model.opt.learning_rate,\n",
    "    # weight_decay=cfg.model.opt.weight_decay,\n",
    "    # # backbone\n",
    "    backbone=backbone,\n",
    "    hyperparam_ignore=[\"backbone\"],\n",
    ")\n",
    "model.head.set_train_mode(\"linear\")\n",
    "callbacks = [(\n",
    "        pl.callbacks.LambdaCallback(\n",
    "            on_train_epoch_start=(\n",
    "                lambda trainer, pl_module: (\n",
    "                    model.head.set_train_mode(\"cnn\")\n",
    "                    if trainer.current_epoch\n",
    "                    > cfg.model.virtualeve.epochs_linear\n",
    "                    else None\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name     | Type                                 | Params | Mode \n",
      "--------------------------------------------------------------------------\n",
      "0 | backbone | MAE                                  | 104 M  | eval \n",
      "1 | encoder  | WrapEncoder                          | 104 M  | eval \n",
      "2 | decoder  | ConvTransformerTokensToEmbeddingNeck | 78.1 K | train\n",
      "3 | head     | HybridIrradianceModel                | 10.8 M | train\n",
      "--------------------------------------------------------------------------\n",
      "80.5 K    Trainable params\n",
      "115 M     Non-trainable params\n",
      "115 M     Total params\n",
      "461.597   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05e24abb60e45dbaf2c6329a33def20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90de0152fc24978b7141d3938e18f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer \n",
    "os.environ['PJRT_DEVICE'] = 'GPU'\n",
    "trainer = Trainer(max_epochs=2, precision=32, callbacks=callbacks)\n",
    "trainer.fit(model=model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgements\n",
    "This work is the research product of the SDO-FM: A Multi-Modal Foundation Model POC for SDO (Grant#: 80NSSC24K0701); funded and supported by NASA. The research and its outputs have been designed, managed and delivered by Trillium Technologies Inc.\n",
    "\n",
    "**Authors**\n",
    "\n",
    "James Walsh, University of Cambridge  \n",
    "Daniel Gass, University of Central Lancashire  \n",
    "Raul Ramos Pollan, Universidad Industrial de Santander  \n",
    "Richard Galvez, Pure Storage  \n",
    "Paul Wright, Dublin Institute for Advanced Studies  \n",
    "Atılım Güneş Baydin, University of Oxford  \n",
    "Noah Kasmanoff, AE Studio   \n",
    "Jason Naradowsky, University of Tokyo  \n",
    "\n",
    "PI: Anne Spalding, Trillium Technolgies Inc  \n",
    "Co-I: James Parr, Trillium Technologies Inc "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDOFM Demo",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
