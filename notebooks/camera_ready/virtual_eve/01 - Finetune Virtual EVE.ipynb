{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual EVE From Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background:\n",
    "This notebook provides an example of finetuning with SDOFM. In this case we create a virtual eve instrument, starting the training from a SDOFM pretrained foundation model, accomplishing a production ready model much faster than training from scratch.\n",
    "\n",
    "#### Foundation Models\n",
    "The process is akin to that of transfer learning, a method typically used in computer vision, for example by freezing a feature extracting neural network pretrained on imagenet. For an extensive treatment of the method of transfer learning, as it was considered before the advent of large modern models, please see [review paper](https://arxiv.org/abs/1811.08883).\n",
    "\n",
    "For the sake of conceptual understanding we can think of the foundation model as a feature extractor depicted in the figure below as the \"Head\". \n",
    "\n",
    "This head model is pretrained on a large dataset, in this case SDO, and the motivation here is that the foundation head model has a built-in understanding of the dataset it was pretrained on. \n",
    "\n",
    "During the pretraining step, the foundation model is unfrozen, and the entire model is fine-tuned on the foundational dataset. This process is called transfer learning, and it is used to train models on smaller datasets, where training from scratch would not be feasible. This is our approach in this notebook.\n",
    "\n",
    "![Figure 1: Architectural Diagram](assets/architecture_figure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "For this section, please be sure to be located in the project root directory before executing any commands. None of the cells in this section are meant to be ran from the notebook IDE, but rather your terminal.\n",
    "\n",
    "#### System Requirements\n",
    "This tutorial assumes that you have conda or miniconda installed and are on a linux or macos machine. It's advisable to install miniconda if you have to decide between the two (smaller install), however, if you already have conda installed, you can skip on to the next step.\n",
    "\n",
    "For instructions on installing miniconda, please see [Miniconda Installation](https://docs.anaconda.com/miniconda/miniconda-install/).\n",
    "\n",
    "##### Python environment setup\n",
    "After you're sure you have conda installed on your system, please run the following command from the project root directory to install a new conda environment\n",
    "\n",
    "`conda env create -f notebooks/camera_ready/virtual_eve/conda_env.yml` .\n",
    "\n",
    "And activate the newly created environment with:\n",
    "\n",
    "`conda activate virtual-eve-finetuning`.\n",
    "\n",
    "Next, install the required python libraries:\n",
    "\n",
    "`pip install -r notebooks/camera_ready/virtual_eve/requirements.txt`\n",
    "\n",
    "And the local sdofm package:\n",
    "\n",
    "`pip install -e .`\n",
    "\n",
    "Lastly, make sure to select the correct python kernel associated with this environment, (likely located in `${CONDA_PREFIX_1}/envs/virtual-eve-finetuning/bin/python`)\n",
    "\n",
    "Nice, you should now be all set to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the finetuning\n",
    "\n",
    "We begin by importing the newly installed libraries we will need tp run this notebook. Note: the import cell below is the first one you should be executing in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import omegaconf\n",
    "from sdofm.datasets import SDOMLDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the appropriate configuration file for this run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = omegaconf.OmegaConf.load(\"finetune_virtualeve_config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration file specifies various parameters for the model run such as \n",
    "\n",
    "- Experiment Config: Where to find the opensource pretrained model weights\n",
    "- Data configuration: where to load the input data from, data metadata, etc\n",
    "- Run parameters: Number of epochs, etc\n",
    "- Misc: log levels, etc.\n",
    "\n",
    "We can interrogate the values in the configuration file by either opening the configuration file itself, or printing the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_level': 'DEBUG',\n",
       " 'experiment': {'name': None, 'project': 'sdofm', 'task': 'finetune', 'model': 'virtualeve', 'resuming': False, 'checkpoint': None, 'backbone': {'checkpoint': 'model-tk45el88:best', 'model': 'mae'}, 'seed': 0, 'disable_cuda': False, 'wandb': {'enable': True, 'entity': 'fdlx', 'group': 'sdofm-phase1', 'job_type': 'finetune', 'tags': [], 'notes': '', 'output_directory': 'wandb_output', 'log_model': 'all'}, 'gcp_storage': {'enabled': True, 'bucket': 'sdofm-checkpoints'}, 'fold': None, 'evaluate': False, 'device': None, 'precision': 'bf16-true', 'log_n_batches': 1000, 'save_results': True, 'accelerator': 'auto', 'profiler': None, 'distributed': {'enabled': True, 'world_size': 'auto', 'strategy': 'ddp_find_unused_parameters_true'}, 'log_every_n_steps': 5},\n",
       " 'data': {'min_date': '2011-10-01 00:00:00.00', 'max_date': '2011-12-31 23:59:59.99', 'month_splits': {'val': [11], 'test': [12], 'holdout': []}, 'num_workers': 32, 'prefetch_factor': 3, 'num_frames': 1, 'drop_frame_dim': False, 'sdoml': {'base_directory': '/mnt/sdoml', 'sub_directory': {'hmi': 'HMI.zarr', 'aia': 'AIA.zarr', 'eve': 'EVE_legacy.zarr', 'cache': 'cache'}, 'components': None, 'wavelengths': None, 'ions': None, 'frequency': '12min', 'mask_with_hmi_threshold': None}},\n",
       " 'model': {'mae': {'img_size': 512, 'patch_size': 16, 'num_frames': 1, 'tubelet_size': 1, 'in_chans': 9, 'embed_dim': 512, 'depth': 24, 'num_heads': 16, 'decoder_embed_dim': 512, 'decoder_depth': 8, 'decoder_num_heads': 16, 'mlp_ratio': 4.0, 'norm_layer': 'LayerNorm', 'norm_pix_loss': False, 'masking_ratio': 0.5}, 'samae': {'masking_type': 'random', 'active_region_mu_degs': 15.73, 'active_region_std_degs': 6.14, 'active_region_scale': 1.0, 'active_region_abs_lon_max_degs': 60, 'active_region_abs_lat_max_degs': 60}, 'nvae': {'use_se': True, 'res_dist': True, 'num_x_bits': 8, 'num_latent_scales': 3, 'num_groups_per_scale': 1, 'num_latent_per_group': 1, 'ada_groups': True, 'min_groups_per_scale': 1, 'num_channels_enc': 30, 'num_channels_dec': 30, 'num_preprocess_blocks': 2, 'num_preprocess_cells': 2, 'num_cell_per_cond_enc': 2, 'num_postprocess_blocks': 2, 'num_postprocess_cells': 2, 'num_cell_per_cond_dec': 2, 'num_mixture_dec': 1, 'num_nf': 2, 'kl_anneal_portion': 0.3, 'kl_const_portion': 0.0001, 'kl_const_coeff': 0.0001, 'weight_decay_norm_anneal': True, 'weight_decay_norm_init': 1.0, 'weight_decay_norm': 0.01}, 'autocalibration': {'freeze_encoder': True, 'num_neck_filters': 32, 'output_dim': 1, 'loss': 'mse'}, 'virtualeve': {'freeze_encoder': True, 'num_neck_filters': 32, 'cnn_model': 'efficientnet_b3', 'lr_linear': 0.01, 'lr_cnn': 0.0001, 'cnn_dp': 0.75, 'epochs_linear': 20}, 'opt': {'loss': 'mse', 'scheduler': 'constant', 'scheduler_warmup': 0, 'batch_size': 16, 'learning_rate': 0.0001, 'weight_decay': 0.0003, 'optimiser': 'adam', 'epochs': 50, 'patience': 2}},\n",
       " 'hydra': {'mode': 'RUN'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the SDOMLDataModule, which defines how we interact with the training dataset for the finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[* CACHE SYSTEM *] Found cached index data in /mnt/sdoml/cache/aligndata_AIA_FULL_12min.csv.\n",
      "[* CACHE SYSTEM *] Found cached normalization data in /mnt/sdoml/cache/normalizations_AIA_FULL_12min.json.\n",
      "[* CACHE SYSTEM *] Found cached HMI mask data in /mnt/sdoml/cache/hmi_mask_512x512.npy.\n"
     ]
    }
   ],
   "source": [
    "data_module = SDOMLDataModule(\n",
    "    hmi_path=None,\n",
    "    aia_path=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.aia\n",
    "    ),\n",
    "    eve_path=None,\n",
    "    components=cfg.data.sdoml.components,\n",
    "    wavelengths=cfg.data.sdoml.wavelengths,\n",
    "    ions=cfg.data.sdoml.ions,\n",
    "    frequency=cfg.data.sdoml.frequency,\n",
    "    batch_size=cfg.model.opt.batch_size,\n",
    "    num_workers=cfg.data.num_workers,\n",
    "    val_months=cfg.data.month_splits.val,\n",
    "    test_months=cfg.data.month_splits.test,\n",
    "    holdout_months=cfg.data.month_splits.holdout,\n",
    "    cache_dir=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.cache\n",
    "    ),\n",
    "    min_date=cfg.data.min_date,\n",
    "    max_date=cfg.data.max_date,\n",
    "    num_frames=1,\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VirtualEveModel(BaseModel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            # Backbone parameters\n",
    "            img_size: int = 512,\n",
    "            patch_size: int = 16,\n",
    "            embed_dim: int = 128,\n",
    "            num_frames: int = 5,\n",
    "            # Neck parameters\n",
    "            num_neck_filters: int = 32,\n",
    "            # Head parameters\n",
    "            # d_input=None,\n",
    "            cnn_model: str = \"efficientnet_b3\",\n",
    "            lr_linear: float = 0.01,\n",
    "            lr_cnn: float = 0.0001,\n",
    "            cnn_dp: float = 0.75,\n",
    "            epochs_linear: int = 50,\n",
    "            d_output=None,\n",
    "            eve_norm=None,\n",
    "            # for finetuning\n",
    "            backbone: object = None,\n",
    "            freeze_encoder: bool = True,\n",
    "            # all else\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        ):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.eve_norm = eve_norm\n",
    "\n",
    "            self.backbone = backbone\n",
    "            self.encoder = WrapEncoder(self.backbone)\n",
    "\n",
    "            if freeze_encoder:\n",
    "                self.encoder.eval()\n",
    "                for param in self.encoder.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            num_tokens = img_size // patch_size\n",
    "\n",
    "            # NECK\n",
    "            self.decoder = ConvTransformerTokensToEmbeddingNeck(\n",
    "                embed_dim=embed_dim,\n",
    "                output_embed_dim=num_neck_filters,\n",
    "                Hp=num_tokens,\n",
    "                Wp=num_tokens,\n",
    "                drop_cls_token=True,\n",
    "                num_frames=num_frames,\n",
    "            )\n",
    "\n",
    "            # HEAD\n",
    "            self.head = HybridIrradianceModel(\n",
    "                # virtual eve\n",
    "                d_input=num_neck_filters,\n",
    "                d_output=d_output,\n",
    "                eve_norm=eve_norm,\n",
    "                # from config\n",
    "                cnn_model=cnn_model,\n",
    "                lr_linear=lr_linear,\n",
    "                lr_cnn=lr_cnn,\n",
    "                cnn_dp=cnn_dp,\n",
    "                epochs_linear=epochs_linear,\n",
    "            )\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            imgs, eve = batch\n",
    "            x = self.encoder(imgs[:, :9, :, :, :])\n",
    "            y_hat = self.head(self.decoder(x))\n",
    "            loss = self.head.loss_func(y_hat, eve[:, :38])\n",
    "            self.log(\"train_loss\", loss)\n",
    "            return loss\n",
    "\n",
    "        def validation_step(self, batch, batch_idx):\n",
    "            imgs, eve = batch\n",
    "            x = self.encoder(imgs[:, :9, :, :, :])\n",
    "            y_hat = self.head(self.decoder(x))\n",
    "            loss = self.head.loss_func(y_hat, eve[:, :38])\n",
    "            self.log(\"val_loss\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
