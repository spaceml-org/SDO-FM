{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual EVE From Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "TODO\n",
    "\n",
    "### Background\n",
    "This notebook provides an example of finetuning with SDOFM. In this case we create a virtual eve instrument, starting the training from a SDOFM pretrained foundation model, accomplishing a production ready model much faster than training from scratch.\n",
    "\n",
    "#### Foundation Models\n",
    "The process is akin to that of transfer learning, a method typically used in computer vision, for example by freezing a feature extracting neural network pretrained on imagenet. For an extensive treatment of the method of transfer learning, as it was considered before the advent of large modern models, please see [review paper](https://arxiv.org/abs/1811.08883).\n",
    "\n",
    "For the sake of conceptual understanding we can think of the foundation model as a feature extractor depicted in the figure below as the \"Head\". \n",
    "\n",
    "This head model is pretrained on a large dataset, in this case SDO, and the motivation here is that the foundation head model has a built-in understanding of the dataset it was pretrained on. \n",
    "\n",
    "During the pretraining step, the foundation model is unfrozen, and the entire model is fine-tuned on the foundational dataset. This process is called transfer learning, and it is used to train models on smaller datasets, where training from scratch would not be feasible. This is our approach in this notebook.\n",
    "\n",
    "![Figure 1: Architectural Diagram](assets/architecture_diag.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "For this section, please be sure to be located in the project root directory before executing any commands. None of the cells in this section are meant to be ran from the notebook IDE, but rather your terminal.\n",
    "\n",
    "#### System Requirements\n",
    "This tutorial assumes that you have conda or miniconda installed and are on a linux or macos machine. It's advisable to install miniconda if you have to decide between the two (smaller install), however, if you already have conda installed, you can skip on to the next step.\n",
    "\n",
    "For instructions on installing miniconda, please see [Miniconda Installation](https://docs.anaconda.com/miniconda/miniconda-install/).\n",
    "\n",
    "##### Python environment setup\n",
    "After you're sure you have conda installed on your system, please run the following command from the project root directory to install a new conda environment\n",
    "\n",
    "```bash\n",
    "conda env create -f notebooks/camera_ready/virtual_eve/conda_env.yml\n",
    "\n",
    "# Activate the newly created environment with:\n",
    "conda activate virtual-eve-finetuning\n",
    "\n",
    "# Next, install the required python libraries:\n",
    "pip install -r notebooks/camera_ready/virtual_eve/requirements.txt\n",
    "\n",
    "# Also the local sdofm package:\n",
    "pip install -e .\n",
    "```\n",
    "Lastly, make sure to select the correct python kernel associated with this environment, (likely located in `${CONDA_PREFIX_1}/envs/virtual-eve-finetuning/bin/python`)\n",
    "\n",
    "Nice, you should now be all set to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the finetuning\n",
    "\n",
    "We begin by importing the newly installed libraries we will need tp run this notebook. Note: the import cell below is the first one you should be executing in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import omegaconf\n",
    "from sdofm.datasets import SDOMLDataModule\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the appropriate configuration file for this run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = omegaconf.OmegaConf.load(\"finetune_virtualeve_config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration file specifies various parameters for the model run such as \n",
    "\n",
    "- Experiment Config: Where to find the opensource pretrained model weights\n",
    "- Data configuration: where to load the input data from, data metadata, etc\n",
    "- Run parameters: Number of epochs, etc\n",
    "- Misc: log levels, etc.\n",
    "\n",
    "We can interrogate the values in the configuration file by either opening the configuration file itself, or printing the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_level': 'DEBUG',\n",
       " 'experiment': {'name': None, 'project': 'sdofm', 'task': 'finetune', 'model': 'virtualeve', 'resuming': False, 'checkpoint': None, 'backbone': {'checkpoint': 'assets/model.ckpt', 'model': 'mae'}, 'seed': 0, 'disable_cuda': False, 'wandb': {'enable': False, 'entity': '', 'group': '', 'job_type': 'finetune', 'tags': [], 'notes': '', 'output_directory': 'wandb_output', 'log_model': 'all'}, 'gcp_storage': {'enabled': True, 'bucket': 'sdofm-checkpoints'}, 'fold': None, 'evaluate': False, 'device': None, 'precision': 'bf16-true', 'log_n_batches': 1000, 'save_results': True, 'accelerator': 'auto', 'profiler': None, 'distributed': {'enabled': True, 'world_size': 'auto', 'strategy': 'ddp_find_unused_parameters_true'}, 'log_every_n_steps': 5},\n",
       " 'data': {'min_date': '2011-10-01 00:00:00.00', 'max_date': '2011-12-31 23:59:59.99', 'month_splits': {'val': [11], 'test': [12], 'holdout': []}, 'num_workers': 4, 'prefetch_factor': 3, 'num_frames': 1, 'drop_frame_dim': True, 'sdoml': {'base_directory': '/mnt/sdoml', 'sub_directory': {'hmi': 'HMI.zarr', 'aia': 'AIA.zarr', 'eve': 'EVE_legacy.zarr', 'cache': 'cache'}, 'components': None, 'wavelengths': None, 'ions': None, 'frequency': '12min', 'mask_with_hmi_threshold': None, 'feature_engineering': {'enabled': False}}},\n",
       " 'model': {'mae': {'img_size': 512, 'patch_size': 16, 'num_frames': 1, 'tubelet_size': 1, 'in_chans': 9, 'embed_dim': 512, 'depth': 24, 'num_heads': 16, 'decoder_embed_dim': 512, 'decoder_depth': 8, 'decoder_num_heads': 16, 'mlp_ratio': 4.0, 'norm_layer': 'LayerNorm', 'norm_pix_loss': False, 'masking_ratio': 0.5}, 'samae': {'masking_type': 'random', 'active_region_mu_degs': 15.73, 'active_region_std_degs': 6.14, 'active_region_scale': 1.0, 'active_region_abs_lon_max_degs': 60, 'active_region_abs_lat_max_degs': 60}, 'nvae': {'use_se': True, 'res_dist': True, 'num_x_bits': 8, 'num_latent_scales': 3, 'num_groups_per_scale': 1, 'num_latent_per_group': 1, 'ada_groups': True, 'min_groups_per_scale': 1, 'num_channels_enc': 30, 'num_channels_dec': 30, 'num_preprocess_blocks': 2, 'num_preprocess_cells': 2, 'num_cell_per_cond_enc': 2, 'num_postprocess_blocks': 2, 'num_postprocess_cells': 2, 'num_cell_per_cond_dec': 2, 'num_mixture_dec': 1, 'num_nf': 2, 'kl_anneal_portion': 0.3, 'kl_const_portion': 0.0001, 'kl_const_coeff': 0.0001, 'weight_decay_norm_anneal': True, 'weight_decay_norm_init': 1.0, 'weight_decay_norm': 0.01}, 'autocalibration': {'freeze_encoder': True, 'num_neck_filters': 32, 'output_dim': 1, 'loss': 'mse'}, 'virtualeve': {'freeze_encoder': True, 'num_neck_filters': 32, 'cnn_model': 'efficientnet_b3', 'lr_linear': 0.01, 'lr_cnn': 0.0001, 'cnn_dp': 0.75, 'epochs_linear': 2}, 'opt': {'loss': 'mse', 'scheduler': 'constant', 'scheduler_warmup': 0, 'batch_size': 1, 'learning_rate': 0.0001, 'weight_decay': 0.0003, 'optimiser': 'adam', 'epochs': 5, 'patience': 2}},\n",
       " 'hydra': {'mode': 'RUN'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the SDOMLDataModule, which defines how we interact with the training dataset for the finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[* CACHE SYSTEM *] Found cached index data in /mnt/sdoml/cache/aligndata_HMI_FULL_AIA_FULL_EVE_FULL_12min.csv.\n",
      "[* CACHE SYSTEM *] Found cached normalization data in /mnt/sdoml/cache/normalizations_HMI_FULL_AIA_FULL_EVE_FULL_12min.json.\n",
      "[* CACHE SYSTEM *] Found cached HMI mask data in /mnt/sdoml/cache/hmi_mask_512x512.npy.\n"
     ]
    }
   ],
   "source": [
    "data_module = SDOMLDataModule(\n",
    "     hmi_path=(\n",
    "         os.path.join(\n",
    "            cfg.data.sdoml.base_directory,\n",
    "            cfg.data.sdoml.sub_directory.hmi,\n",
    "        )\n",
    "        if cfg.data.sdoml.sub_directory.hmi\n",
    "        else None\n",
    "    ),\n",
    "    aia_path=(\n",
    "        os.path.join(\n",
    "            cfg.data.sdoml.base_directory,\n",
    "            cfg.data.sdoml.sub_directory.aia,\n",
    "        )\n",
    "        if cfg.data.sdoml.sub_directory.aia\n",
    "        else None\n",
    "    ),\n",
    "    eve_path=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.eve\n",
    "    ),\n",
    "    components=cfg.data.sdoml.components,\n",
    "    wavelengths=cfg.data.sdoml.wavelengths,\n",
    "    ions=cfg.data.sdoml.ions,\n",
    "    frequency=cfg.data.sdoml.frequency,\n",
    "    batch_size=cfg.model.opt.batch_size,\n",
    "    num_workers=cfg.data.num_workers,\n",
    "    val_months=cfg.data.month_splits.val,\n",
    "    test_months=cfg.data.month_splits.test,\n",
    "    holdout_months=cfg.data.month_splits.holdout,\n",
    "    cache_dir=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.cache\n",
    "    ),\n",
    "    min_date=cfg.data.min_date,\n",
    "    max_date=cfg.data.max_date,\n",
    "    num_frames=cfg.data.num_frames,\n",
    "    drop_frame_dim=cfg.data.drop_frame_dim,\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(data_module.train_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a finetuning model\n",
    "Now time to get serious, this model will become our model's \"head.\" The objective of this component is to take now a set of finetuned embeddings and have them predict our true science task. This model was created during FDL-X 2023 and is used as an quick example. It has a switching mode that transitions the model from linear to influenced by a CNN after a defned number of epochs. We're going to do this with Pytorch Lighning for keep hardware agnostic. \n",
    "\n",
    "We first import necessary components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from sdofm import BaseModule\n",
    "import sys\n",
    "from sdofm.models.virtualeve import unnormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearIrradianceModel(BaseModule):\n",
    "    def __init__(self, d_input, d_output, eve_norm):\n",
    "        super().__init__()\n",
    "        # self.eve_norm = eve_norm\n",
    "        self.n_channels = d_input\n",
    "        self.outSize = d_output\n",
    "\n",
    "        self.model = nn.Linear(2 * self.n_channels, self.outSize)\n",
    "        self.loss_func = nn.HuberLoss()  # consider MSE\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_irradiance = torch.mean(x, dim=(2, 3))\n",
    "        std_irradiance = torch.std(x, dim=(2, 3))\n",
    "        x = self.model(torch.cat((mean_irradiance, std_irradiance), dim=1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the CNN efficientnet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNIrradianceModel(BaseModule):\n",
    "    def __init__(self, d_input, d_output, eve_norm, dp=0.75):\n",
    "        super().__init__()\n",
    "\n",
    "        model = torchvision.models.efficientnet_b3(weights=\"IMAGENET1K_V1\")\n",
    "        conv1_out = model.features[0][0].out_channels\n",
    "        model.features[0][0] = nn.Conv2d(\n",
    "            d_input,\n",
    "            conv1_out,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(2, 2),\n",
    "            padding=(1, 1),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        lin_in = model.classifier[1].in_features\n",
    "        classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dp, inplace=True),\n",
    "            nn.Linear(in_features=lin_in, out_features=d_output, bias=True),\n",
    "        )\n",
    "        model.classifier = classifier\n",
    "\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__.startswith(\"Dropout\"):\n",
    "                m.p = dp\n",
    "\n",
    "        self.model = model\n",
    "        self.loss_func = nn.HuberLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the hybrid switching model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridIrradianceModel(BaseModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input,\n",
    "        d_output,\n",
    "        eve_norm,\n",
    "        *args,\n",
    "        lr_linear=0.01,\n",
    "        lr_cnn=0.0001,\n",
    "        cnn_dp=0.75,\n",
    "        ln_params=None,  # used in lambda function out of scope?\n",
    "        epochs_linear=None,  # out of scope\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # self.eve_norm = torch.Tensor(self.eve_norm).float() #eve_norm\n",
    "        self.register_buffer(\"eve_norm\", torch.Tensor(eve_norm).float())\n",
    "        self.n_channels = d_input\n",
    "        self.outSize = d_output\n",
    "        # self.ln_params = ln_params # unused\n",
    "        self.lr_linear = lr_linear\n",
    "        self.lr_cnn = lr_cnn\n",
    "        self.train_mode = \"linear\"\n",
    "\n",
    "        self.ln_model = LinearIrradianceModel(d_input, d_output, eve_norm)\n",
    "        self.cnn_model = CNNIrradianceModel(\n",
    "            d_input, d_output, eve_norm, dp=cnn_dp\n",
    "        )\n",
    "        self.loss_func = nn.HuberLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ln_model.forward(x) + self.cnn_lambda * self.cnn_model.forward(x)\n",
    "\n",
    "    def forward_unnormalize(self, x):\n",
    "        return self.unnormalize(self.forward(x))\n",
    "\n",
    "    def unnormalize(self, y):\n",
    "        # eve_norm = torch.tensor(self.eve_norm).float()\n",
    "        norm_mean = self.eve_norm[0]\n",
    "        norm_stdev = self.eve_norm[1]\n",
    "        y = y * norm_stdev[None] + norm_mean[None]  # .to(y) .to(y)\n",
    "        return y\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.loss_func(y_pred, y)\n",
    "\n",
    "        # print(\"t: trying un\")\n",
    "        y = self.unnormalize(y)\n",
    "        y_pred = self.unnormalize(y_pred)\n",
    "        # print(\"t: success un\")\n",
    "\n",
    "        epsilon = sys.float_info.epsilon\n",
    "        # computing relative absolute error\n",
    "        rae = torch.abs((y - y_pred) / (torch.abs(y) + epsilon)) * 100\n",
    "        av_rae = rae.mean()\n",
    "\n",
    "        self.log_everything(\"train\", loss, av_rae, float(self.cnn_lambda))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.loss_func(y_pred, y)\n",
    "\n",
    "        y = self.unnormalize(y)\n",
    "        y_pred = self.unnormalize(y_pred)\n",
    "\n",
    "        epsilon = sys.float_info.epsilon\n",
    "        # computing relative absolute error\n",
    "        rae = torch.abs((y - y_pred) / (torch.abs(y) + epsilon)) * 100\n",
    "        av_rae = rae.mean()\n",
    "        av_rae_wl = rae.mean(0)\n",
    "        # compute average cross-correlation\n",
    "        cc = torch.tensor(\n",
    "            [\n",
    "                torch.corrcoef(torch.stack([y[i], y_pred[i]]))[0, 1]\n",
    "                for i in range(y.shape[0])\n",
    "            ]\n",
    "        ).mean()\n",
    "        # compute mean absolute error\n",
    "        mae = torch.abs(y - y_pred).mean()\n",
    "\n",
    "        self.log_everything(\n",
    "            \"val\", loss, av_rae, float(self.cnn_lambda), av_rae_wl, mae, cc\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.loss_func(y_pred, y)\n",
    "\n",
    "        y = self.unnormalize(y)\n",
    "        y_pred = self.unnormalize(y_pred)\n",
    "\n",
    "        epsilon = sys.float_info.epsilon\n",
    "        rae = torch.abs((y - y_pred) / (torch.abs(y) + epsilon)) * 100\n",
    "        av_rae = rae.mean()\n",
    "        av_rae_wl = rae.mean(0)\n",
    "        # compute average cross-correlation\n",
    "        cc = torch.tensor(\n",
    "            [\n",
    "                torch.corrcoef(torch.stack([y[i], y_pred[i]]))[0, 1]\n",
    "                for i in range(y.shape[0])\n",
    "            ]\n",
    "        ).mean()\n",
    "        # mean absolute error\n",
    "        mae = torch.abs(y - y_pred).mean()\n",
    "\n",
    "        self.log_everything(\n",
    "            \"test\", loss, av_rae, float(self.cnn_lambda), av_rae_wl, mae, cc\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def log_everything(\n",
    "        self, mode, loss, av_rae, train_lambda_cnn, av_rae_wl=None, mae=None, cc=None\n",
    "    ):\n",
    "        # the .to() calls are a fix for https://github.com/Lightning-AI/pytorch-lightning/issues/18803\n",
    "        self.log(\n",
    "            f\"{mode}_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            f\"{mode}_RAE\",\n",
    "            av_rae,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": self.ln_model.parameters()},\n",
    "                {\"params\": self.cnn_model.parameters(), \"lr\": self.lr_cnn},\n",
    "            ],\n",
    "            lr=self.lr_linear,\n",
    "        )\n",
    "\n",
    "    def set_train_mode(self, mode):\n",
    "        if mode == \"linear\":\n",
    "            self.train_mode = \"linear\"\n",
    "            self.cnn_lambda = 0.0\n",
    "            self.cnn_model.freeze()\n",
    "            self.ln_model.unfreeze()\n",
    "        elif mode == \"cnn\":\n",
    "            self.train_mode = \"cnn\"\n",
    "            self.cnn_lambda = 0.01\n",
    "            self.cnn_model.unfreeze()\n",
    "            self.ln_model.freeze()\n",
    "        else:\n",
    "            raise NotImplemented(f\"Mode not supported: {mode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the backbone model. In this example we're loading a Masked Autoencoder and an NVAE for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using <class 'sdofm.datasets.SDOML.SDOMLDataModule'> Data Class\n",
      "[* CACHE SYSTEM *] Found cached index data in /mnt/sdoml/cache/aligndata_AIA_FULL_12min.csv.\n",
      "[* CACHE SYSTEM *] Found cached normalization data in /mnt/sdoml/cache/normalizations_AIA_FULL_12min.json.\n",
      "[* CACHE SYSTEM *] Found cached HMI mask data in /mnt/sdoml/cache/hmi_mask_512x512.npy.\n",
      "Loading checkpoint...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from pretrain import Pretrainer\n",
    "MAE = Pretrainer(cfg, logger=None, is_backbone=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the full model\n",
    "Now that we have a suitable head model completing our objective, predicting EVE from the AIA instrument we can construct our full model. This is broken down by the sections as defined in the figure above. The backbone is set into finetuning mode and the encoder is explictly frozen. These outputs are fed into a decoding layer before they are ready for non-transformer models, such as our example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = MAE.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdofm.models import WrapEncoder, ConvTransformerTokensToEmbeddingNeck\n",
    "\n",
    "class VirtualEVELatentModel(BaseModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            # Backbone parameters\n",
    "            img_size: int = 512,\n",
    "            patch_size: int = 16,\n",
    "            embed_dim: int = 128,\n",
    "            num_frames: int = 1,\n",
    "            # Neck parameters\n",
    "            num_neck_filters: int = 32,\n",
    "            # Head parameters\n",
    "            # d_input=None,\n",
    "            cnn_model: str = \"efficientnet_b3\",\n",
    "            lr_linear: float = 0.01,\n",
    "            lr_cnn: float = 0.0001,\n",
    "            cnn_dp: float = 0.75,\n",
    "            epochs_linear: int = 50,\n",
    "            d_output=None,\n",
    "            eve_norm=None,\n",
    "            # for finetuning\n",
    "            backbone: object = None,\n",
    "            use_latents: bool = True,\n",
    "            # all else\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        ):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.eve_norm = eve_norm\n",
    "\n",
    "            self.backbone = backbone\n",
    "            # self.encoder = WrapEncoder(self.backbone)\n",
    "\n",
    "            if use_latents:\n",
    "                self.encoder.eval()\n",
    "                for param in self.encoder.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            num_tokens = img_size // patch_size\n",
    "\n",
    "            # NECK\n",
    "            self.decoder = ConvTransformerTokensToEmbeddingNeck(\n",
    "                embed_dim=embed_dim,\n",
    "                output_embed_dim=num_neck_filters,\n",
    "                Hp=num_tokens,\n",
    "                Wp=num_tokens,\n",
    "                drop_cls_token=True,\n",
    "                num_frames=num_frames,\n",
    "            )\n",
    "\n",
    "            # HEAD\n",
    "            self.head = HybridIrradianceModel(\n",
    "                # virtual eve\n",
    "                d_input=num_neck_filters,\n",
    "                d_output=d_output,\n",
    "                eve_norm=eve_norm,\n",
    "                # from config\n",
    "                # cnn_model=cnn_model,\n",
    "                lr_linear=lr_linear,\n",
    "                lr_cnn=lr_cnn,\n",
    "                cnn_dp=cnn_dp,\n",
    "                # epochs_linear=epochs_linear,\n",
    "            )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, eve = batch\n",
    "        x = self.encoder(imgs[:, :9, :, :])\n",
    "        y_hat = self.head(self.decoder(x))\n",
    "        loss = self.head.loss_func(y_hat, eve[:, :38, 0])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, eve = batch\n",
    "        x = self.encoder(imgs[:, :9, :, :])\n",
    "        y_hat = self.head(self.decoder(x))\n",
    "        # print(eve.shape)\n",
    "        loss = self.head.loss_func(y_hat, eve[:, :38, 0])\n",
    "        self.log(\"val_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdofm.models import WrapEncoder, ConvTransformerTokensToEmbeddingNeck\n",
    "\n",
    "class VirtualEVEModel(BaseModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            # Backbone parameters\n",
    "            img_size: int = 512,\n",
    "            patch_size: int = 16,\n",
    "            embed_dim: int = 128,\n",
    "            num_frames: int = 1,\n",
    "            # Neck parameters\n",
    "            num_neck_filters: int = 32,\n",
    "            # Head parameters\n",
    "            # d_input=None,\n",
    "            cnn_model: str = \"efficientnet_b3\",\n",
    "            lr_linear: float = 0.01,\n",
    "            lr_cnn: float = 0.0001,\n",
    "            cnn_dp: float = 0.75,\n",
    "            epochs_linear: int = 50,\n",
    "            d_output=None,\n",
    "            eve_norm=None,\n",
    "            # for finetuning\n",
    "            backbone: object = None,\n",
    "            freeze_encoder: bool = True,\n",
    "            # all else\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        ):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.eve_norm = eve_norm\n",
    "\n",
    "            self.backbone = backbone\n",
    "\n",
    "            if freeze_encoder:\n",
    "                self.backbone.autoencoder.blocks.eval()\n",
    "                for param in self.backbone.autoencoder.blocks.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            num_tokens = img_size // patch_size\n",
    "\n",
    "            # HEAD\n",
    "            self.head = HybridIrradianceModel(\n",
    "                # virtual eve\n",
    "                d_input=9, # for number of wavelengths\n",
    "                d_output=d_output,\n",
    "                eve_norm=eve_norm,\n",
    "                # from config\n",
    "                # cnn_model=cnn_model,\n",
    "                lr_linear=lr_linear,\n",
    "                lr_cnn=lr_cnn,\n",
    "                cnn_dp=cnn_dp,\n",
    "                # epochs_linear=epochs_linear,\n",
    "            )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, eve = batch\n",
    "        x = self.backbone(imgs[:, :9, :, :])\n",
    "        # y_hat = self.head(self.decoder(x))\n",
    "        print(x.shape)\n",
    "        loss = self.head.loss_func(y_hat, eve[:, :38, 0])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, eve = batch\n",
    "        x = self.encoder(imgs[:, :9, :, :])\n",
    "        y_hat = self.head(self.decoder(x))\n",
    "        # print(eve.shape)\n",
    "        loss = self.head.loss_func(y_hat, eve[:, :38, 0])\n",
    "        self.log(\"val_loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then initialised with the params as defined in the experiment configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.57 GiB of which 10.75 MiB is free. Process 2803765 has 13.78 GiB memory in use. Including non-PyTorch memory, this process has 796.00 MiB memory in use. Of the allocated memory 658.19 MiB is allocated by PyTorch, and 21.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01 - Finetune Virtual EVE.ipynb Cell 30\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m backbone_params[\u001b[39m\"\u001b[39m\u001b[39membed_dim\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmae\u001b[39m.\u001b[39membed_dim\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m backbone_params[\u001b[39m\"\u001b[39m\u001b[39mnum_frames\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmae\u001b[39m.\u001b[39mnum_frames\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m VirtualEVEModel(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# backbone\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbackbone_params,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# virtual eve params\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcfg\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvirtualeve,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     d_output\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(data_module\u001b[39m.\u001b[39;49mions),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     eve_norm\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49marray(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         data_module\u001b[39m.\u001b[39;49mnormalizations[\u001b[39m\"\u001b[39;49m\u001b[39mEVE\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39meve_norm\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# general\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# optimiser=cfg.model.opt.optimiser,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# lr=cfg.model.opt.learning_rate,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# weight_decay=cfg.model.opt.weight_decay,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# # backbone\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     backbone\u001b[39m=\u001b[39;49mbackbone,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     hyperparam_ignore\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mbackbone\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m model\u001b[39m.\u001b[39mhead\u001b[39m.\u001b[39mset_train_mode(\u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m callbacks \u001b[39m=\u001b[39m [(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m         pl\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mLambdaCallback(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m             on_train_epoch_start\u001b[39m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     )]\n",
      "\u001b[1;32m/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01 - Finetune Virtual EVE.ipynb Cell 30\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m num_tokens \u001b[39m=\u001b[39m img_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m patch_size\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# HEAD\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead \u001b[39m=\u001b[39m HybridIrradianceModel(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m# virtual eve\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     d_input\u001b[39m=\u001b[39;49m\u001b[39m9\u001b[39;49m, \u001b[39m# for number of wavelengths\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     d_output\u001b[39m=\u001b[39;49md_output,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     eve_norm\u001b[39m=\u001b[39;49meve_norm,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39m# from config\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# cnn_model=cnn_model,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     lr_linear\u001b[39m=\u001b[39;49mlr_linear,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     lr_cnn\u001b[39m=\u001b[39;49mlr_cnn,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     cnn_dp\u001b[39m=\u001b[39;49mcnn_dp,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39m# epochs_linear=epochs_linear,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m )\n",
      "\u001b[1;32m/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01 - Finetune Virtual EVE.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     d_input,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m ):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# self.eve_norm = torch.Tensor(self.eve_norm).float() #eve_norm\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnvae-train-t4-dsexport/home/walsh/SDO-FM/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39meve_norm\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mTensor(eve_norm)\u001b[39m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/SDO-FM/sdofm/BaseModule.py:17\u001b[0m, in \u001b[0;36mBaseModule.__init__\u001b[0;34m(self, optimiser, lr, weight_decay, hyperparam_ignore, *args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m      7\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m      8\u001b[0m     optimiser: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     15\u001b[0m ):\n\u001b[1;32m     16\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 17\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_hyperparameters(ignore\u001b[39m=\u001b[39;49mhyperparam_ignore)\n\u001b[1;32m     19\u001b[0m     \u001b[39m# optimiser values\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimiser \u001b[39m=\u001b[39m optimiser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/core/mixins/hparams_mixin.py:130\u001b[0m, in \u001b[0;36mHyperparametersMixin.save_hyperparameters\u001b[0;34m(self, ignore, frame, logger, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m current_frame:\n\u001b[1;32m    129\u001b[0m         frame \u001b[39m=\u001b[39m current_frame\u001b[39m.\u001b[39mf_back\n\u001b[0;32m--> 130\u001b[0m save_hyperparameters(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, ignore\u001b[39m=\u001b[39;49mignore, frame\u001b[39m=\u001b[39;49mframe, given_hparams\u001b[39m=\u001b[39;49mgiven_hparams)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:214\u001b[0m, in \u001b[0;36msave_hyperparameters\u001b[0;34m(obj, ignore, frame, given_hparams, *args)\u001b[0m\n\u001b[1;32m    208\u001b[0m         rank_zero_warn(\n\u001b[1;32m    209\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttribute \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m!r}\u001b[39;00m\u001b[39m is an instance of `nn.Module` and is already saved during checkpointing.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    210\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m It is recommended to ignore them using `self.save_hyperparameters(ignore=[\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m!r}\u001b[39;00m\u001b[39m])`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    213\u001b[0m \u001b[39m# make a deep copy so there are no other runtime changes reflected\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m obj\u001b[39m.\u001b[39m_hparams_initial \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39;49mdeepcopy(obj\u001b[39m.\u001b[39;49m_hparams)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[39m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "    \u001b[0;31m[... skipping similar frames: deepcopy at line 172 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[39m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "    \u001b[0;31m[... skipping similar frames: deepcopy at line 172 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "    \u001b[0;31m[... skipping similar frames: deepcopy at line 172 (8 times), _reconstruct at line 297 (4 times), _deepcopy_dict at line 231 (3 times), _reconstruct at line 271 (3 times), deepcopy at line 146 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[39m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__deepcopy__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[39m=\u001b[39m copier(memo)\n\u001b[1;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[39m=\u001b[39m dispatch_table\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:59\u001b[0m, in \u001b[0;36mParameter.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m memo[\u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m)]\n\u001b[1;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mclone(memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad)\n\u001b[1;32m     60\u001b[0m     memo[\u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m)] \u001b[39m=\u001b[39m result\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.57 GiB of which 10.75 MiB is free. Process 2803765 has 13.78 GiB memory in use. Including non-PyTorch memory, this process has 796.00 MiB memory in use. Of the allocated memory 658.19 MiB is allocated by PyTorch, and 21.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "\n",
    "backbone_params = {}\n",
    "backbone_params[\"img_size\"] = cfg.model.mae.img_size\n",
    "backbone_params[\"patch_size\"] = cfg.model.mae.patch_size\n",
    "backbone_params[\"embed_dim\"] = cfg.model.mae.embed_dim\n",
    "backbone_params[\"num_frames\"] = cfg.model.mae.num_frames\n",
    "\n",
    "model = VirtualEVEModel(\n",
    "    # backbone\n",
    "    **backbone_params,\n",
    "    # virtual eve params\n",
    "    **cfg.model.virtualeve,\n",
    "    d_output=len(data_module.ions),\n",
    "    eve_norm=np.array(\n",
    "        data_module.normalizations[\"EVE\"][\"eve_norm\"],\n",
    "        dtype=np.float32,\n",
    "    ),\n",
    "    # general\n",
    "    # optimiser=cfg.model.opt.optimiser,\n",
    "    # lr=cfg.model.opt.learning_rate,\n",
    "    # weight_decay=cfg.model.opt.weight_decay,\n",
    "    # # backbone\n",
    "    backbone=backbone,\n",
    "    hyperparam_ignore=[\"backbone\"],\n",
    ")\n",
    "model.head.set_train_mode(\"linear\")\n",
    "callbacks = [(\n",
    "        pl.callbacks.LambdaCallback(\n",
    "            on_train_epoch_start=(\n",
    "                lambda trainer, pl_module: (\n",
    "                    model.head.set_train_mode(\"cnn\")\n",
    "                    if trainer.current_epoch\n",
    "                    > cfg.model.virtualeve.epochs_linear\n",
    "                    else None\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = backbone.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE.model.autoencoder.blocks.eval()\n",
    "for param in MAE.model.autoencoder.blocks.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104553728"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103504128"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in MAE.model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27846912"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in MAE.model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type                  | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | backbone | MAE                   | 104 M  | train\n",
      "1 | head     | HybridIrradianceModel | 10.8 M | train\n",
      "-----------------------------------------------------------\n",
      "722       Trainable params\n",
      "115 M     Non-trainable params\n",
      "115 M     Total params\n",
      "461.245   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebff509c2534b00beb298f733e4ecc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer \n",
    "os.environ['PJRT_DEVICE'] = 'GPU'\n",
    "trainer = Trainer(max_epochs=2, precision=32, callbacks=callbacks)\n",
    "trainer.fit(model=model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgements\n",
    "This work is the research product of the SDO-FM: A Multi-Modal Foundation Model POC for SDO (Grant#: 80NSSC24K0701); funded and supported by NASA. The research and its outputs have been designed, managed and delivered by Trillium Technologies Inc.\n",
    "\n",
    "**Authors**\n",
    "\n",
    "James Walsh, University of Cambridge  \n",
    "Daniel Gass, University of Central Lancashire  \n",
    "Raul Ramos Pollan, Universidad Industrial de Santander  \n",
    "Richard Galvez, Pure Storage  \n",
    "Paul Wright, Dublin Institute for Advanced Studies  \n",
    "Atılım Güneş Baydin, University of Oxford  \n",
    "Noah Kasmanoff, AE Studio   \n",
    "Jason Naradowsky, University of Tokyo  \n",
    "\n",
    "PI: Anne Spalding, Trillium Technolgies Inc  \n",
    "Co-I: James Parr, Trillium Technologies Inc "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
