{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual EVE From Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background:\n",
    "This notebook provides an example of finetuning with SDOFM. In this case we create a virtual eve instrument, starting the training from a SDOFM pretrained foundation model, accomplishing a production ready model much faster than training from scratch.\n",
    "\n",
    "#### Foundation Models\n",
    "The process is akin to that of transfer learning, a method typically used in computer vision, for example by freezing a feature extracting neural network pretrained on imagenet. For an extensive treatment of the method of transfer learning, as it was considered before the advent of large modern models, please see [review paper](https://arxiv.org/abs/1811.08883).\n",
    "\n",
    "For the sake of conceptual understanding we can think of the foundation model as a feature extractor, and the head as a classifier. The foundation model is pretrained on a large dataset, and the head is trained on a smaller dataset. The foundation model is frozen during the training of the head, and the head is trained on the smaller dataset. The foundation model is then unfrozen, and the entire model is fine-tuned on the smaller dataset. This process is called transfer learning, and it is used to train models on smaller datasets, where training from scratch would not be feasible. This is our approach in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "For this section, please be sure to be located in the project root directory before executing any commands. None of the cells in this section are meant to be ran from the notebook IDE, but rather your terminal.\n",
    "\n",
    "#### System Requirements\n",
    "This tutorial assumes that you have conda or miniconda installed and are on a linux or macos machine. It's advisable to install miniconda if you have to decide between the two (smaller install), however, if you already have conda installed, you can skip on to the next step.\n",
    "\n",
    "For instructions on installing miniconda, please see [Miniconda Installation](https://docs.anaconda.com/miniconda/miniconda-install/).\n",
    "\n",
    "##### Python environment setup\n",
    "After you're sure you have conda installed on your system, please run the following command from the project root directory to install a new conda environment\n",
    "\n",
    "`conda env create -f notebooks/camera_ready/virtual_eve/conda_env.yml` .\n",
    "\n",
    "And activate the newly created environment with:\n",
    "\n",
    "`conda activate virtual-eve-finetuning`.\n",
    "\n",
    "Next, install the required python libraries:\n",
    "\n",
    "`pip install -r notebooks/camera_ready/virtual_eve/requirements.txt`\n",
    "\n",
    "And the local sdofm package:\n",
    "\n",
    "`pip install -e .`\n",
    "\n",
    "Lastly, make sure to select the correct python kernel associated with this environment, (likely located in `${CONDA_PREFIX_1}/envs/virtual-eve-finetuning/bin/python`)\n",
    "\n",
    "Nice, you should now be all set to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the finetuning\n",
    "\n",
    "We begin by importing the newly installed libraries we will need tp run this notebook. Note: the import cell below is the first one you should be executing in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import omegaconf\n",
    "from sdofm.datasets import SDOMLDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the appropriate configuration file for this run. This configuration file specifies various parameters for the model run such as where to find the opensource pretrained model weights, where to load the input data from, as well as some run parameters such as number of epochs, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'omegaconf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43momegaconf\u001b[49m\u001b[38;5;241m.\u001b[39mOmegaConf\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetune_virtualeve_config.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'omegaconf' is not defined"
     ]
    }
   ],
   "source": [
    "cfg = omegaconf.OmegaConf.load(\"finetune_virtualeve_config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[* CACHE SYSTEM *] Found cached index data in /mnt/sdoml/cache/aligndata_AIA_FULL_12min.csv.\n",
      "[* CACHE SYSTEM *] Found cached normalization data in /mnt/sdoml/cache/normalizations_AIA_FULL_12min.json.\n",
      "[* CACHE SYSTEM *] Found cached HMI mask data in /mnt/sdoml/cache/hmi_mask_512x512.npy.\n"
     ]
    }
   ],
   "source": [
    "data_module = SDOMLDataModule(\n",
    "    hmi_path=None,\n",
    "    aia_path=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.aia\n",
    "    ),\n",
    "    eve_path=None,\n",
    "    components=cfg.data.sdoml.components,\n",
    "    wavelengths=cfg.data.sdoml.wavelengths,\n",
    "    ions=cfg.data.sdoml.ions,\n",
    "    frequency=cfg.data.sdoml.frequency,\n",
    "    batch_size=cfg.model.opt.batch_size,\n",
    "    num_workers=cfg.data.num_workers,\n",
    "    val_months=cfg.data.month_splits.val,\n",
    "    test_months=cfg.data.month_splits.test,\n",
    "    holdout_months=cfg.data.month_splits.holdout,\n",
    "    cache_dir=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.cache\n",
    "    ),\n",
    "    min_date=cfg.data.min_date,\n",
    "    max_date=cfg.data.max_date,\n",
    "    num_frames=1,\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(\n",
    "        self,\n",
    "        # Backbone parameters\n",
    "        img_size: int = 512,\n",
    "        patch_size: int = 16,\n",
    "        embed_dim: int = 128,\n",
    "        num_frames: int = 5,\n",
    "        # Neck parameters\n",
    "        num_neck_filters: int = 32,\n",
    "        # Head parameters\n",
    "        # d_input=None,\n",
    "        cnn_model: str = \"efficientnet_b3\",\n",
    "        lr_linear: float = 0.01,\n",
    "        lr_cnn: float = 0.0001,\n",
    "        cnn_dp: float = 0.75,\n",
    "        epochs_linear: int = 50,\n",
    "        d_output=None,\n",
    "        eve_norm=None,\n",
    "        # for finetuning\n",
    "        backbone: object = None,\n",
    "        freeze_encoder: bool = True,\n",
    "        # all else\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.eve_norm = eve_norm\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.encoder = WrapEncoder(self.backbone)\n",
    "\n",
    "        if freeze_encoder:\n",
    "            self.encoder.eval()\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        num_tokens = img_size // patch_size\n",
    "\n",
    "        # NECK\n",
    "        self.decoder = ConvTransformerTokensToEmbeddingNeck(\n",
    "            embed_dim=embed_dim,\n",
    "            output_embed_dim=num_neck_filters,\n",
    "            Hp=num_tokens,\n",
    "            Wp=num_tokens,\n",
    "            drop_cls_token=True,\n",
    "            num_frames=num_frames,\n",
    "        )\n",
    "\n",
    "        # HEAD\n",
    "        self.head = HybridIrradianceModel(\n",
    "            # virtual eve\n",
    "            d_input=num_neck_filters,\n",
    "            d_output=d_output,\n",
    "            eve_norm=eve_norm,\n",
    "            # from config\n",
    "            cnn_model=cnn_model,\n",
    "            lr_linear=lr_linear,\n",
    "            lr_cnn=lr_cnn,\n",
    "            cnn_dp=cnn_dp,\n",
    "            epochs_linear=epochs_linear,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, eve = batch\n",
    "        x = self.encoder(imgs[:, :9, :, :, :])\n",
    "        y_hat = self.head(self.decoder(x))\n",
    "        loss = self.head.loss_func(y_hat, eve[:, :38])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, eve = batch\n",
    "        x = self.encoder(imgs[:, :9, :, :, :])\n",
    "        y_hat = self.head(self.decoder(x))\n",
    "        loss = self.head.loss_func(y_hat, eve[:, :38])\n",
    "        self.log(\"val_loss\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
