{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual EVE From Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background:\n",
    "This notebook provides an example of finetuning with SDOFM. In this case we create a virtual eve instrument, starting the training from a SDOFM pretrained foundation model, accomplishing a production ready model much faster than training from scratch.\n",
    "\n",
    "#### Foundation Models\n",
    "The process is akin to that of transfer learning, a method typically used in computer vision, for example by freezing a feature extracting neural network pretrained on imagenet. For an extensive treatment of the method of transfer learning, as it was considered before the advent of large modern models, please see [review paper](https://arxiv.org/abs/1811.08883).\n",
    "\n",
    "For the sake of conceptual understanding we can think of the foundation model as a feature extractor, and the head as a classifier. The foundation model is pretrained on a large dataset, and the head is trained on a smaller dataset. The foundation model is frozen during the training of the head, and the head is trained on the smaller dataset. The foundation model is then unfrozen, and the entire model is fine-tuned on the smaller dataset. This process is called transfer learning, and it is used to train models on smaller datasets, where training from scratch would not be feasible. This is our approach in this notebook.\n",
    "\n",
    "![Figure 1: Architectural Diagram](assets/architecture_figure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "For this section, please be sure to be located in the project root directory before executing any commands. None of the cells in this section are meant to be ran from the notebook IDE, but rather your terminal.\n",
    "\n",
    "#### System Requirements\n",
    "This tutorial assumes that you have conda or miniconda installed and are on a linux or macos machine. It's advisable to install miniconda if you have to decide between the two (smaller install), however, if you already have conda installed, you can skip on to the next step.\n",
    "\n",
    "For instructions on installing miniconda, please see [Miniconda Installation](https://docs.anaconda.com/miniconda/miniconda-install/).\n",
    "\n",
    "##### Python environment setup\n",
    "After you're sure you have conda installed on your system, please run the following command from the project root directory to install a new conda environment\n",
    "\n",
    "`conda env create -f notebooks/camera_ready/virtual_eve/conda_env.yml` .\n",
    "\n",
    "And activate the newly created environment with:\n",
    "\n",
    "`conda activate virtual-eve-finetuning`.\n",
    "\n",
    "Next, install the required python libraries:\n",
    "\n",
    "`pip install -r notebooks/camera_ready/virtual_eve/requirements.txt`\n",
    "\n",
    "And the local sdofm package:\n",
    "\n",
    "`pip install -e .`\n",
    "\n",
    "Lastly, make sure to select the correct python kernel associated with this environment, (likely located in `${CONDA_PREFIX_1}/envs/virtual-eve-finetuning/bin/python`)\n",
    "\n",
    "Nice, you should now be all set to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the finetuning\n",
    "\n",
    "We begin by importing the newly installed libraries we will need tp run this notebook. Note: the import cell below is the first one you should be executing in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import omegaconf\n",
    "from sdofm.datasets import SDOMLDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the appropriate configuration file for this run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = omegaconf.OmegaConf.load(\"finetune_virtualeve_config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration file specifies various parameters for the model run such as \n",
    "\n",
    "- Experiment Config: Where to find the opensource pretrained model weights\n",
    "- Data configuration: where to load the input data from, data metadata, etc\n",
    "- Run parameters: Number of epochs, etc\n",
    "- Misc: log levels, etc.\n",
    "\n",
    "We can interrogate the values in the configuration file by either opening the configuration file itself, or printing the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_level': 'DEBUG',\n",
       " 'experiment': {'name': None, 'project': 'sdofm', 'task': 'finetune', 'model': 'virtualeve', 'resuming': False, 'checkpoint': None, 'backbone': {'checkpoint': 'model-tk45el88:best', 'model': 'mae'}, 'seed': 0, 'disable_cuda': False, 'wandb': {'enable': True, 'entity': 'fdlx', 'group': 'sdofm-phase1', 'job_type': 'finetune', 'tags': [], 'notes': '', 'output_directory': 'wandb_output', 'log_model': 'all'}, 'gcp_storage': {'enabled': True, 'bucket': 'sdofm-checkpoints'}, 'fold': None, 'evaluate': False, 'device': None, 'precision': 'bf16-true', 'log_n_batches': 1000, 'save_results': True, 'accelerator': 'auto', 'profiler': None, 'distributed': {'enabled': True, 'world_size': 'auto', 'strategy': 'ddp_find_unused_parameters_true'}, 'log_every_n_steps': 5},\n",
       " 'data': {'min_date': '2011-10-01 00:00:00.00', 'max_date': '2011-12-31 23:59:59.99', 'month_splits': {'val': [11], 'test': [12], 'holdout': []}, 'num_workers': 32, 'prefetch_factor': 3, 'num_frames': 1, 'drop_frame_dim': False, 'sdoml': {'base_directory': '/mnt/sdoml', 'sub_directory': {'hmi': 'HMI.zarr', 'aia': 'AIA.zarr', 'eve': 'EVE_legacy.zarr', 'cache': 'cache'}, 'components': None, 'wavelengths': None, 'ions': None, 'frequency': '12min', 'mask_with_hmi_threshold': None}},\n",
       " 'model': {'mae': {'img_size': 512, 'patch_size': 16, 'num_frames': 1, 'tubelet_size': 1, 'in_chans': 9, 'embed_dim': 512, 'depth': 24, 'num_heads': 16, 'decoder_embed_dim': 512, 'decoder_depth': 8, 'decoder_num_heads': 16, 'mlp_ratio': 4.0, 'norm_layer': 'LayerNorm', 'norm_pix_loss': False, 'masking_ratio': 0.5}, 'samae': {'masking_type': 'random', 'active_region_mu_degs': 15.73, 'active_region_std_degs': 6.14, 'active_region_scale': 1.0, 'active_region_abs_lon_max_degs': 60, 'active_region_abs_lat_max_degs': 60}, 'nvae': {'use_se': True, 'res_dist': True, 'num_x_bits': 8, 'num_latent_scales': 3, 'num_groups_per_scale': 1, 'num_latent_per_group': 1, 'ada_groups': True, 'min_groups_per_scale': 1, 'num_channels_enc': 30, 'num_channels_dec': 30, 'num_preprocess_blocks': 2, 'num_preprocess_cells': 2, 'num_cell_per_cond_enc': 2, 'num_postprocess_blocks': 2, 'num_postprocess_cells': 2, 'num_cell_per_cond_dec': 2, 'num_mixture_dec': 1, 'num_nf': 2, 'kl_anneal_portion': 0.3, 'kl_const_portion': 0.0001, 'kl_const_coeff': 0.0001, 'weight_decay_norm_anneal': True, 'weight_decay_norm_init': 1.0, 'weight_decay_norm': 0.01}, 'autocalibration': {'freeze_encoder': True, 'num_neck_filters': 32, 'output_dim': 1, 'loss': 'mse'}, 'virtualeve': {'freeze_encoder': True, 'num_neck_filters': 32, 'cnn_model': 'efficientnet_b3', 'lr_linear': 0.01, 'lr_cnn': 0.0001, 'cnn_dp': 0.75, 'epochs_linear': 20}, 'opt': {'loss': 'mse', 'scheduler': 'constant', 'scheduler_warmup': 0, 'batch_size': 16, 'learning_rate': 0.0001, 'weight_decay': 0.0003, 'optimiser': 'adam', 'epochs': 50, 'patience': 2}},\n",
       " 'hydra': {'mode': 'RUN'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the SDOMLDataModule, which defines how we interact with the training dataset for the finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'.zgroup'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/virtual-eve-finetuning/lib/python3.12/site-packages/zarr/storage.py:1141\u001b[0m, in \u001b[0;36mDirectoryStore.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1141\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/mnt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_module \u001b[38;5;241m=\u001b[39m \u001b[43mSDOMLDataModule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhmi_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43maia_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msdoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msdoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_directory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maia\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meve_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomponents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msdoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwavelengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msdoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwavelengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msdoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msdoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_months\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonth_splits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_months\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonth_splits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mholdout_months\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonth_splits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mholdout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msdoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msdoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_directory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m data_module\u001b[38;5;241m.\u001b[39msetup()\n",
      "File \u001b[0;32m~/repos/trillium/SDO-FM/sdofm/datasets/SDOML.py:362\u001b[0m, in \u001b[0;36mSDOMLDataModule.__init__\u001b[0;34m(self, hmi_path, aia_path, eve_path, components, wavelengths, ions, frequency, batch_size, num_workers, val_months, test_months, holdout_months, cache_dir, apply_mask, num_frames, drop_frame_dim, min_date, max_date)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# checking if AIA is in the dataset\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misAIA:\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maia_data \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDirectoryStore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maia_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwavelengths \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwavelengths \u001b[38;5;241m=\u001b[39m ALL_WAVELENGTHS\n",
      "File \u001b[0;32m~/miniconda3/envs/virtual-eve-finetuning/lib/python3.12/site-packages/zarr/hierarchy.py:1477\u001b[0m, in \u001b[0;36mgroup\u001b[0;34m(store, overwrite, chunk_store, cache_attrs, synchronizer, path, zarr_version, meta_array)\u001b[0m\n\u001b[1;32m   1474\u001b[0m     requires_init \u001b[38;5;241m=\u001b[39m overwrite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contains_group(store, path)\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_init:\n\u001b[0;32m-> 1477\u001b[0m     \u001b[43minit_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Group(\n\u001b[1;32m   1480\u001b[0m     store,\n\u001b[1;32m   1481\u001b[0m     read_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     meta_array\u001b[38;5;241m=\u001b[39mmeta_array,\n\u001b[1;32m   1488\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/virtual-eve-finetuning/lib/python3.12/site-packages/zarr/storage.py:682\u001b[0m, in \u001b[0;36minit_group\u001b[0;34m(store, overwrite, path, chunk_store)\u001b[0m\n\u001b[1;32m    679\u001b[0m     store[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzarr.json\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39m_metadata_class\u001b[38;5;241m.\u001b[39mencode_hierarchy_metadata(\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# initialise metadata\u001b[39;00m\n\u001b[0;32m--> 682\u001b[0m \u001b[43m_init_group_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_version \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# TODO: Should initializing a v3 group also create a corresponding\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;66;03m#       empty folder under data/root/? I think probably not until there\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;66;03m#       is actual data written there.\u001b[39;00m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/virtual-eve-finetuning/lib/python3.12/site-packages/zarr/storage.py:743\u001b[0m, in \u001b[0;36m_init_group_metadata\u001b[0;34m(store, overwrite, path, chunk_store)\u001b[0m\n\u001b[1;32m    741\u001b[0m key \u001b[38;5;241m=\u001b[39m _prefix_to_group_key(store, _path_to_prefix(path))\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(store, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_metadata_class\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 743\u001b[0m     \u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39m_metadata_class\u001b[38;5;241m.\u001b[39mencode_group_metadata(meta)\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    745\u001b[0m     store[key] \u001b[38;5;241m=\u001b[39m encode_group_metadata(meta)\n",
      "File \u001b[0;32m~/miniconda3/envs/virtual-eve-finetuning/lib/python3.12/site-packages/zarr/storage.py:1144\u001b[0m, in \u001b[0;36mDirectoryStore.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m errno\u001b[38;5;241m.\u001b[39mEEXIST:\n\u001b[0;32m-> 1144\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# write to temporary file\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;66;03m# note we're not using tempfile.NamedTemporaryFile to avoid restrictive file permissions\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m temp_name \u001b[38;5;241m=\u001b[39m file_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.partial\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: '.zgroup'"
     ]
    }
   ],
   "source": [
    "data_module = SDOMLDataModule(\n",
    "    hmi_path=None,\n",
    "    aia_path=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.aia\n",
    "    ),\n",
    "    eve_path=None,\n",
    "    components=cfg.data.sdoml.components,\n",
    "    wavelengths=cfg.data.sdoml.wavelengths,\n",
    "    ions=cfg.data.sdoml.ions,\n",
    "    frequency=cfg.data.sdoml.frequency,\n",
    "    batch_size=cfg.model.opt.batch_size,\n",
    "    num_workers=cfg.data.num_workers,\n",
    "    val_months=cfg.data.month_splits.val,\n",
    "    test_months=cfg.data.month_splits.test,\n",
    "    holdout_months=cfg.data.month_splits.holdout,\n",
    "    cache_dir=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.cache\n",
    "    ),\n",
    "    min_date=cfg.data.min_date,\n",
    "    max_date=cfg.data.max_date,\n",
    "    num_frames=1,\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VirtualEveModel(BaseModel):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            # Backbone parameters\n",
    "            img_size: int = 512,\n",
    "            patch_size: int = 16,\n",
    "            embed_dim: int = 128,\n",
    "            num_frames: int = 5,\n",
    "            # Neck parameters\n",
    "            num_neck_filters: int = 32,\n",
    "            # Head parameters\n",
    "            # d_input=None,\n",
    "            cnn_model: str = \"efficientnet_b3\",\n",
    "            lr_linear: float = 0.01,\n",
    "            lr_cnn: float = 0.0001,\n",
    "            cnn_dp: float = 0.75,\n",
    "            epochs_linear: int = 50,\n",
    "            d_output=None,\n",
    "            eve_norm=None,\n",
    "            # for finetuning\n",
    "            backbone: object = None,\n",
    "            freeze_encoder: bool = True,\n",
    "            # all else\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        ):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.eve_norm = eve_norm\n",
    "\n",
    "            self.backbone = backbone\n",
    "            self.encoder = WrapEncoder(self.backbone)\n",
    "\n",
    "            if freeze_encoder:\n",
    "                self.encoder.eval()\n",
    "                for param in self.encoder.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            num_tokens = img_size // patch_size\n",
    "\n",
    "            # NECK\n",
    "            self.decoder = ConvTransformerTokensToEmbeddingNeck(\n",
    "                embed_dim=embed_dim,\n",
    "                output_embed_dim=num_neck_filters,\n",
    "                Hp=num_tokens,\n",
    "                Wp=num_tokens,\n",
    "                drop_cls_token=True,\n",
    "                num_frames=num_frames,\n",
    "            )\n",
    "\n",
    "            # HEAD\n",
    "            self.head = HybridIrradianceModel(\n",
    "                # virtual eve\n",
    "                d_input=num_neck_filters,\n",
    "                d_output=d_output,\n",
    "                eve_norm=eve_norm,\n",
    "                # from config\n",
    "                cnn_model=cnn_model,\n",
    "                lr_linear=lr_linear,\n",
    "                lr_cnn=lr_cnn,\n",
    "                cnn_dp=cnn_dp,\n",
    "                epochs_linear=epochs_linear,\n",
    "            )\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            imgs, eve = batch\n",
    "            x = self.encoder(imgs[:, :9, :, :, :])\n",
    "            y_hat = self.head(self.decoder(x))\n",
    "            loss = self.head.loss_func(y_hat, eve[:, :38])\n",
    "            self.log(\"train_loss\", loss)\n",
    "            return loss\n",
    "\n",
    "        def validation_step(self, batch, batch_idx):\n",
    "            imgs, eve = batch\n",
    "            x = self.encoder(imgs[:, :9, :, :, :])\n",
    "            y_hat = self.head(self.decoder(x))\n",
    "            loss = self.head.loss_func(y_hat, eve[:, :38])\n",
    "            self.log(\"val_loss\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
