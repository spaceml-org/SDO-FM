{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual EVE From Pretrained Embeddings\n",
    "\n",
    "## Purpose:\n",
    "This notebook provides an example of finetuning with SDOFM. In this case we create a virtual eve instrument, starting the training from a SDOFM pretrained foundation model, accomplishing a production ready model much faster than training from scratch.\n",
    "\n",
    "## Foundation Models\n",
    "The process is akin to that of transfer learning, a method typically used in computer vision, for example by freezing a feature extracting neural network pretrained on imagenet. For an extensive treatment of the method of transfer learning, as it was considered before the advent of large modern models, please see [review paper](https://arxiv.org/abs/1811.08883).\n",
    "\n",
    "For the sake of conceptual understanding we can think of the foundation model as a feature extractor, and the head as a classifier. The foundation model is pretrained on a large dataset, and the head is trained on a smaller dataset. The foundation model is frozen during the training of the head, and the head is trained on the smaller dataset. The foundation model is then unfrozen, and the entire model is fine-tuned on the smaller dataset. This process is called transfer learning, and it is used to train models on smaller datasets, where training from scratch would not be feasible. This is our approach in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the libraries we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import wandb\n",
    "import omegaconf\n",
    "from sdofm import utils\n",
    "from sdofm.datasets import SDOMLDataModule, DegradedSDOMLDataModule\n",
    "from sdofm.pretraining import MAE, SAMAE\n",
    "from sdofm.finetuning import Autocalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = omegaconf.OmegaConf.load(\"../../experiments/finetune_32.2M_mae_virtualeve.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[* CACHE SYSTEM *] Found cached index data in /mnt/sdoml/cache/aligndata_AIA_FULL_12min.csv.\n",
      "[* CACHE SYSTEM *] Found cached normalization data in /mnt/sdoml/cache/normalizations_AIA_FULL_12min.json.\n",
      "[* CACHE SYSTEM *] Found cached HMI mask data in /mnt/sdoml/cache/hmi_mask_512x512.npy.\n"
     ]
    }
   ],
   "source": [
    "data_module = SDOMLDataModule(\n",
    "    hmi_path=None,\n",
    "    aia_path=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.aia\n",
    "    ),\n",
    "    eve_path=None,\n",
    "    components=cfg.data.sdoml.components,\n",
    "    wavelengths=cfg.data.sdoml.wavelengths,\n",
    "    ions=cfg.data.sdoml.ions,\n",
    "    frequency=cfg.data.sdoml.frequency,\n",
    "    batch_size=cfg.model.opt.batch_size,\n",
    "    num_workers=cfg.data.num_workers,\n",
    "    val_months=cfg.data.month_splits.val,\n",
    "    test_months=cfg.data.month_splits.test,\n",
    "    holdout_months=cfg.data.month_splits.holdout,\n",
    "    cache_dir=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.cache\n",
    "    ),\n",
    "    min_date=cfg.data.min_date,\n",
    "    max_date=cfg.data.max_date,\n",
    "    num_frames=1,\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(\n",
    "        self,\n",
    "        # Backbone parameters\n",
    "        img_size: int = 512,\n",
    "        patch_size: int = 16,\n",
    "        embed_dim: int = 128,\n",
    "        num_frames: int = 5,\n",
    "        # Neck parameters\n",
    "        num_neck_filters: int = 32,\n",
    "        # Head parameters\n",
    "        # d_input=None,\n",
    "        cnn_model: str = \"efficientnet_b3\",\n",
    "        lr_linear: float = 0.01,\n",
    "        lr_cnn: float = 0.0001,\n",
    "        cnn_dp: float = 0.75,\n",
    "        epochs_linear: int = 50,\n",
    "        d_output=None,\n",
    "        eve_norm=None,\n",
    "        # for finetuning\n",
    "        backbone: object = None,\n",
    "        freeze_encoder: bool = True,\n",
    "        # all else\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.eve_norm = eve_norm\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.encoder = WrapEncoder(self.backbone)\n",
    "\n",
    "        if freeze_encoder:\n",
    "            self.encoder.eval()\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        num_tokens = img_size // patch_size\n",
    "\n",
    "        # NECK\n",
    "        self.decoder = ConvTransformerTokensToEmbeddingNeck(\n",
    "            embed_dim=embed_dim,\n",
    "            output_embed_dim=num_neck_filters,\n",
    "            Hp=num_tokens,\n",
    "            Wp=num_tokens,\n",
    "            drop_cls_token=True,\n",
    "            num_frames=num_frames,\n",
    "        )\n",
    "\n",
    "        # HEAD\n",
    "        self.head = HybridIrradianceModel(\n",
    "            # virtual eve\n",
    "            d_input=num_neck_filters,\n",
    "            d_output=d_output,\n",
    "            eve_norm=eve_norm,\n",
    "            # from config\n",
    "            cnn_model=cnn_model,\n",
    "            lr_linear=lr_linear,\n",
    "            lr_cnn=lr_cnn,\n",
    "            cnn_dp=cnn_dp,\n",
    "            epochs_linear=epochs_linear,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, eve = batch\n",
    "        x = self.encoder(imgs[:, :9, :, :, :])\n",
    "        y_hat = self.head(self.decoder(x))\n",
    "        loss = self.head.loss_func(y_hat, eve[:, :38])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, eve = batch\n",
    "        x = self.encoder(imgs[:, :9, :, :, :])\n",
    "        y_hat = self.head(self.decoder(x))\n",
    "        loss = self.head.loss_func(y_hat, eve[:, :38])\n",
    "        self.log(\"val_loss\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
