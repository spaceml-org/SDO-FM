{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data Reconstruction from Pretrained Embeddings\n",
    "\n",
    "For this example we're going to create a simpler finetuning set up to train a model with one channel corrupted. As we have a pretrained model the decoder is going to attempt to reconstruct our lost data.\n",
    "\n",
    "![Figure 1: Architectural Diagram](assets/architecture_diags_corrupt.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "\n",
    "Load config and datamodule as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: SunpyUserWarning: Importing sunpy.map without its extra dependencies may result in errors.\n",
      "The following packages are not installed:\n",
      "['mpl-animators>=1.0.0', 'reproject>=0.9.0']\n",
      "To install sunpy with these dependencies use `pip install sunpy[map]` or `pip install sunpy[all]` for all extras. \n",
      "If you installed sunpy via conda, please report this to the community channel: https://matrix.to/#/#sunpy:openastronomy.org [sunpy.util.sysinfo]\n",
      "WARNING: SunpyUserWarning: Importing sunpy.visualization without its extra dependencies may result in errors.\n",
      "The following packages are not installed:\n",
      "['mpl-animators>=1.0.0']\n",
      "To install sunpy with these dependencies use `pip install sunpy[visualization]` or `pip install sunpy[all]` for all extras. \n",
      "If you installed sunpy via conda, please report this to the community channel: https://matrix.to/#/#sunpy:openastronomy.org [sunpy.util.sysinfo]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import omegaconf\n",
    "from sdofm.datasets import SDOMLDataModule\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = omegaconf.OmegaConf.load(\"finetune_missing_config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[* CACHE SYSTEM *] Found cached index data in /mnt/sdoml/cache/aligndata_AIA_FULL_12min.csv.\n",
      "[* CACHE SYSTEM *] Found cached normalization data in /mnt/sdoml/cache/normalizations_AIA_FULL_12min.json.\n",
      "[* CACHE SYSTEM *] Found cached HMI mask data in /mnt/sdoml/cache/hmi_mask_512x512.npy.\n"
     ]
    }
   ],
   "source": [
    "data_module = SDOMLDataModule(\n",
    "    hmi_path=None,\n",
    "    aia_path=(\n",
    "        os.path.join(\n",
    "            cfg.data.sdoml.base_directory,\n",
    "            cfg.data.sdoml.sub_directory.aia,\n",
    "        )\n",
    "        if cfg.data.sdoml.sub_directory.aia\n",
    "        else None\n",
    "    ),\n",
    "    eve_path=None,\n",
    "    components=cfg.data.sdoml.components,\n",
    "    wavelengths=cfg.data.sdoml.wavelengths,\n",
    "    ions=cfg.data.sdoml.ions,\n",
    "    frequency=cfg.data.sdoml.frequency,\n",
    "    batch_size=cfg.model.opt.batch_size,\n",
    "    num_workers=cfg.data.num_workers,\n",
    "    val_months=cfg.data.month_splits.val,\n",
    "    test_months=cfg.data.month_splits.test,\n",
    "    holdout_months=cfg.data.month_splits.holdout,\n",
    "    cache_dir=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.cache\n",
    "    ),\n",
    "    min_date=cfg.data.min_date,\n",
    "    max_date=cfg.data.max_date,\n",
    "    num_frames=cfg.data.num_frames,\n",
    "    drop_frame_dim=cfg.data.drop_frame_dim,\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Missing Data Model\n",
    "\n",
    "Here we create a `forward_corrupt_data_override` function to replace our typical training forward pass to rudely set an entire input channel to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdofm.models import WrapEncoder, ConvTransformerTokensToEmbeddingNeck\n",
    "from sdofm.benchmarks import reconstruction as bench_recon\n",
    "import torch.nn.functional as F\n",
    "from sdofm.constants import ALL_WAVELENGTHS\n",
    "from sdofm import BaseModule\n",
    "\n",
    "\n",
    "class MissingDataModel(BaseModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Backbone parameters\n",
    "        img_size: int = 512,\n",
    "        patch_size: int = 16,\n",
    "        embed_dim: int = 128,\n",
    "        num_frames: int = 1,\n",
    "        # for finetuning\n",
    "        backbone: object = None,\n",
    "        freeze_encoder: bool = True,\n",
    "        # all else\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.backbone = backbone\n",
    "\n",
    "        self.masking_ratio = 0.75\n",
    "        self.validation_metrics = []\n",
    "\n",
    "        if freeze_encoder:\n",
    "            self.backbone.autoencoder.blocks.eval()\n",
    "            for param in self.backbone.autoencoder.blocks.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.simulated_corrupt_wavelength = 5\n",
    "\n",
    "        # As this is a reconstruction task, something that the MAE\n",
    "        # was designed to do, we don't require the neck.\n",
    "\n",
    "    def forward_corrupt_data_override(self, imgs, mask_ratio=0.75):\n",
    "        # corrupt our wavelength by setting it all to 0\n",
    "        imgs[:, self.simulated_corrupt_wavelength, :, :] = 0\n",
    "        # continue as normal\n",
    "        latent, mask, ids_restore = self.backbone.autoencoder.forward_encoder(\n",
    "            imgs, mask_ratio\n",
    "        )\n",
    "        pred = self.backbone.autoencoder.forward_decoder(latent, ids_restore)\n",
    "        loss = self.backbone.autoencoder.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch\n",
    "        loss, x_hat, mask = self.forward_corrupt_data_override(\n",
    "            x, mask_ratio=self.masking_ratio\n",
    "        )\n",
    "        x_hat = self.backbone.autoencoder.unpatchify(x_hat)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        loss, x_hat, mask = self.backbone.autoencoder(x, mask_ratio=self.masking_ratio)\n",
    "        x_hat = self.backbone.autoencoder.unpatchify(x_hat)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        for i in range(x.shape[0]):\n",
    "            for frame in range(x.shape[2]):\n",
    "                self.validation_metrics.append(\n",
    "                    bench_recon.get_metrics(\n",
    "                        x[i, :, frame, :, :], x_hat[i, :, frame, :, :], ALL_WAVELENGTHS\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.log(\"val_loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in a pretrained model as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using <class 'sdofm.datasets.SDOML.SDOMLDataModule'> Data Class\n",
      "[* CACHE SYSTEM *] Found cached index data in /mnt/sdoml/cache/aligndata_AIA_FULL_12min.csv.\n",
      "[* CACHE SYSTEM *] Found cached normalization data in /mnt/sdoml/cache/normalizations_AIA_FULL_12min.json.\n",
      "[* CACHE SYSTEM *] Found cached HMI mask data in /mnt/sdoml/cache/hmi_mask_512x512.npy.\n",
      "Loading checkpoint...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from pretrain import Pretrainer\n",
    "\n",
    "MAE = Pretrainer(cfg, logger=None, is_backbone=True)\n",
    "backbone = MAE.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_params = {}\n",
    "backbone_params[\"img_size\"] = cfg.model.mae.img_size\n",
    "backbone_params[\"patch_size\"] = cfg.model.mae.patch_size\n",
    "backbone_params[\"embed_dim\"] = cfg.model.mae.embed_dim\n",
    "backbone_params[\"num_frames\"] = cfg.model.mae.num_frames\n",
    "\n",
    "model = MissingDataModel(\n",
    "    # backbone\n",
    "    **backbone_params,\n",
    "    # backbone\n",
    "    backbone=backbone,\n",
    "    hyperparam_ignore=[\"backbone\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name     | Type | Params | Mode \n",
      "------------------------------------------\n",
      "0 | backbone | MAE  | 104 M  | train\n",
      "------------------------------------------\n",
      "27.8 M    Trainable params\n",
      "76.7 M    Non-trainable params\n",
      "104 M     Total params\n",
      "418.215   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462d1016120945a5b14c8a94c8b60a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9dfb00f12274d8c9310c6298906040c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 22:35:00.801873: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:584] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect with a different incarnation. It has likely restarted.\n",
      "2024-07-23 22:35:00.801980: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:992] /job:jax_worker/replica:0/task:0 has been set to ERROR in coordination service: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect with a different incarnation. It has likely restarted. [type.googleapis.com/tensorflow.CoordinationServiceError='\\\"\\x0c\\n\\njax_worker']\n",
      "2024-07-23 22:35:00.804974: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:584] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt.\n",
      "2024-07-23 22:35:00.805024: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:992] /job:jax_worker/replica:0/task:0 has been set to ERROR in coordination service: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. [type.googleapis.com/tensorflow.CoordinationServiceError='\\\"\\x0c\\n\\njax_worker']\n",
      "2024-07-23 22:35:00.807324: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:584] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt.\n",
      "2024-07-23 22:35:00.807364: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:992] /job:jax_worker/replica:0/task:0 has been set to ERROR in coordination service: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. [type.googleapis.com/tensorflow.CoordinationServiceError='\\\"\\x0c\\n\\njax_worker']\n",
      "2024-07-23 22:35:00.809845: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:584] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt.\n",
      "2024-07-23 22:35:00.809885: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:992] /job:jax_worker/replica:0/task:0 has been set to ERROR in coordination service: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. [type.googleapis.com/tensorflow.CoordinationServiceError='\\\"\\x0c\\n\\njax_worker']\n",
      "2024-07-23 22:35:00.821377: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:584] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt.\n",
      "2024-07-23 22:35:00.821486: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:992] /job:jax_worker/replica:0/task:0 has been set to ERROR in coordination service: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. [type.googleapis.com/tensorflow.CoordinationServiceError='\\\"\\x0c\\n\\njax_worker']\n",
      "2024-07-23 22:35:00.852254: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:584] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt.\n",
      "2024-07-23 22:35:00.852335: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:992] /job:jax_worker/replica:0/task:0 has been set to ERROR in coordination service: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. [type.googleapis.com/tensorflow.CoordinationServiceError='\\\"\\x0c\\n\\njax_worker']\n",
      "2024-07-23 22:35:00.886470: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:584] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt.\n",
      "2024-07-23 22:35:00.886531: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:992] /job:jax_worker/replica:0/task:0 has been set to ERROR in coordination service: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. [type.googleapis.com/tensorflow.CoordinationServiceError='\\\"\\x0c\\n\\njax_worker']\n",
      "2024-07-23 22:35:00.891703: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:584] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt.\n",
      "2024-07-23 22:35:00.891751: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:992] /job:jax_worker/replica:0/task:0 has been set to ERROR in coordination service: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. [type.googleapis.com/tensorflow.CoordinationServiceError='\\\"\\x0c\\n\\njax_worker']\n",
      "2024-07-23 22:35:01.028384: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:584] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt.\n",
      "2024-07-23 22:35:01.028492: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:992] /job:jax_worker/replica:0/task:0 has been set to ERROR in coordination service: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. [type.googleapis.com/tensorflow.CoordinationServiceError='\\\"\\x0c\\n\\njax_worker']\n",
      "2024-07-23 22:35:01.033134: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:584] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt.\n",
      "2024-07-23 22:35:01.033186: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:992] /job:jax_worker/replica:0/task:0 has been set to ERROR in coordination service: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. [type.googleapis.com/tensorflow.CoordinationServiceError='\\\"\\x0c\\n\\njax_worker']\n",
      "2024-07-23 22:35:01.102471: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:584] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt.\n",
      "2024-07-23 22:35:01.102537: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:992] /job:jax_worker/replica:0/task:0 has been set to ERROR in coordination service: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. [type.googleapis.com/tensorflow.CoordinationServiceError='\\\"\\x0c\\n\\njax_worker']\n",
      "2024-07-23 22:35:02.510261: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:584] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt.\n",
      "2024-07-23 22:35:02.510365: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:992] /job:jax_worker/replica:0/task:0 has been set to ERROR in coordination service: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. [type.googleapis.com/tensorflow.CoordinationServiceError='\\\"\\x0c\\n\\njax_worker']\n",
      "2024-07-23 22:35:06.322601: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:584] /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt.\n",
      "2024-07-23 22:35:06.322722: E external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:992] /job:jax_worker/replica:0/task:0 has been set to ERROR in coordination service: ABORTED: /job:jax_worker/replica:0/task:0 unexpectedly tried to connect while it is already in error. ResetTask() should be called before a subsequent connect attempt. [type.googleapis.com/tensorflow.CoordinationServiceError='\\\"\\x0c\\n\\njax_worker']\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "\n",
    "os.environ[\"PJRT_DEVICE\"] = \"GPU\"\n",
    "trainer = Trainer(max_epochs=2, precision=32)\n",
    "trainer.fit(model=model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgements\n",
    "This work is the research product of the SDO-FM: A Multi-Modal Foundation Model POC for SDO (Grant#: 80NSSC24K0701); funded and supported by NASA. The research and its outputs have been designed, managed and delivered by Trillium Technologies Inc.\n",
    "\n",
    "**Authors**\n",
    "\n",
    "James Walsh, University of Cambridge  \n",
    "Daniel Gass, University of Central Lancashire  \n",
    "Raul Ramos Pollan, Universidad Industrial de Santander  \n",
    "Richard Galvez, Pure Storage  \n",
    "Paul Wright, Dublin Institute for Advanced Studies  \n",
    "Atılım Güneş Baydin, University of Oxford  \n",
    "Noah Kasmanoff, AE Studio   \n",
    "Jason Naradowsky, University of Tokyo  \n",
    "\n",
    "PI: Anne Spalding, Trillium Technolgies Inc  \n",
    "Co-I: James Parr, Trillium Technologies Inc "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
