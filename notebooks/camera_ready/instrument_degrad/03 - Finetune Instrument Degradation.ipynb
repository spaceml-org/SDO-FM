{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrument Degradation Prediction from Pretrained Embeddings\n",
    "\n",
    "This example contains a model by Dos Santos et al. to predict instrument degradation. We show here training naively directly on the trained embeddings.\n",
    "\n",
    "[Dos Santos, et al. \"Multichannel autocalibration for the Atmospheric Imaging Assembly using machine learning.\" Astronomy & Astrophysics 648 (2021): A53.](https://www.aanda.org/articles/aa/abs/2021/04/aa40051-20/aa40051-20.html)\n",
    "\n",
    "![Figure 1: Instrument Degrad with latents](assets/architecture_diags_degrad.svg)"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "Please reference: [01-Finetune Virtual EVE notebook](https://github.com/spaceml-org/SDO-FM/blob/main/notebooks/camera_ready/virtual_eve/01%20-%20Finetune%20Virtual%20EVE.ipynb) for environment setup.\n", 
    "Load config and datamodule as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import omegaconf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = omegaconf.OmegaConf.load(\"finetune_degrad_config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[* CACHE SYSTEM *] Found cached index data in /mnt/sdoml/cache/aligndata_AIA_FULL_12min.csv.\n",
      "[* CACHE SYSTEM *] Found cached normalization data in /mnt/sdoml/cache/normalizations_AIA_FULL_12min.json.\n",
      "[* CACHE SYSTEM *] Found cached HMI mask data in /mnt/sdoml/cache/hmi_mask_512x512.npy.\n"
     ]
    }
   ],
   "source": [
    "from sdofm.datasets import DegradedSDOMLDataModule\n",
    "\n",
    "data_module = DegradedSDOMLDataModule(\n",
    "    hmi_path=None,\n",
    "    aia_path=os.path.join(\n",
    "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.aia\n",
    "    ),\n",
    "    eve_path=None,\n",
    "    components=cfg.data.sdoml.components,\n",
    "    wavelengths=cfg.data.sdoml.wavelengths,\n",
    "    ions=cfg.data.sdoml.ions,\n",
    "    frequency=cfg.data.sdoml.frequency,\n",
    "    batch_size=cfg.model.opt.batch_size,\n",
    "    num_workers=cfg.data.num_workers,\n",
    "    val_months=cfg.data.month_splits.val,\n",
    "    test_months=cfg.data.month_splits.test,\n",
    "    holdout_months=cfg.data.month_splits.holdout,\n",
    "    cache_dir=os.path.join(\n",
    "        cfg.data.sdoml.base_directory,\n",
    "        cfg.data.sdoml.sub_directory.cache,\n",
    "    ),\n",
    "    min_date=cfg.data.min_date,\n",
    "    max_date=cfg.data.max_date,\n",
    "    num_frames=cfg.data.num_frames,\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate the Autocalibration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sdofm import BaseModule\n",
    "from sdofm.models import (\n",
    "    Autocalibration13Head,\n",
    "    ConvTransformerTokensToEmbeddingNeck,\n",
    "    MaskedAutoencoderViT3D,\n",
    "    WrapEncoder,\n",
    "    SolarAwareMaskedAutoencoderViT3D,\n",
    ")\n",
    "\n",
    "\n",
    "def heteroscedastic_loss(output, gt_output, reduction):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        output: NN output values, tensor of shape 2, batch_size, n_channels.\n",
    "        where the first dimension contains the mean values and the second\n",
    "        dimension contains the log_var\n",
    "        gt_output: groundtruth values. tensor of shape batch_size, n_channels\n",
    "        reduction: if mean, the loss is averaged across the third dimension,\n",
    "        if summ the loss is summed across the third dimension, if None any\n",
    "        aggregation is performed\n",
    "\n",
    "    Returns:\n",
    "        tensor of size n_channels if reduction is None or tensor of size 0\n",
    "        if reduction is mean or sum\n",
    "\n",
    "    \"\"\"\n",
    "    precision = torch.exp(-output[1])\n",
    "    batch_size = output[0].shape[0]\n",
    "    loss = (\n",
    "        torch.sum(precision * (gt_output - output[0]) ** 2.0 + output[1], 0)\n",
    "        / batch_size\n",
    "    )\n",
    "    if reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return loss.sum()\n",
    "    elif reduction is None:\n",
    "        return loss\n",
    "    else:\n",
    "        raise ValueError(\"Aggregation can only be None, mean or sum.\")\n",
    "\n",
    "\n",
    "class HeteroscedasticLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Heteroscedastic loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction=\"mean\"):\n",
    "        super(HeteroscedasticLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        return heteroscedastic_loss(output, target, reduction=self.reduction)\n",
    "\n",
    "\n",
    "class Autocalibration(BaseModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Backbone parameters\n",
    "        img_size=512,\n",
    "        patch_size=16,\n",
    "        embed_dim=128,\n",
    "        num_frames=5,\n",
    "        # Neck parameters\n",
    "        num_neck_filters: int = 32,\n",
    "        # Head parameters\n",
    "        output_dim: int = 1,\n",
    "        loss: str = \"mse\",\n",
    "        freeze_encoder: bool = True,\n",
    "        # if finetuning\n",
    "        backbone: object = None,\n",
    "        # all else\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.encoder = WrapEncoder(self.backbone)\n",
    "\n",
    "        if freeze_encoder:\n",
    "            self.encoder.eval()\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        num_tokens = img_size // patch_size\n",
    "\n",
    "        # NECK\n",
    "        self.decoder = ConvTransformerTokensToEmbeddingNeck(\n",
    "            embed_dim=embed_dim,\n",
    "            output_embed_dim=num_neck_filters,\n",
    "            Hp=num_tokens,\n",
    "            Wp=num_tokens,\n",
    "            drop_cls_token=True,\n",
    "            num_frames=num_frames,\n",
    "        )\n",
    "\n",
    "        # HEAD\n",
    "        self.head = Autocalibration13Head(\n",
    "            [num_neck_filters, img_size, img_size], output_dim\n",
    "        )\n",
    "\n",
    "        # set loss function\n",
    "        match loss:\n",
    "            case \"mse\":\n",
    "                self.loss_function = nn.MSELoss()\n",
    "            case \"heteroscedastic\":\n",
    "                self.loss_function = HeteroscedasticLoss()\n",
    "            case _:\n",
    "                raise NotImplementedError(f\"Loss function {loss} not implemented\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        degraded_img, degrad_factor, orig_img = batch\n",
    "        x = self.encoder(degraded_img)\n",
    "        x = self.decoder(x)\n",
    "        y_hat = self.head(x)\n",
    "        loss = self.loss_function(y_hat[0, :, :], degrad_factor)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        degraded_img, degrad_factor, orig_img = batch\n",
    "        x = self.encoder(degraded_img)\n",
    "        y_hat = self.head(self.decoder(x))\n",
    "        loss = self.loss_function(y_hat[0, :, :], degrad_factor)\n",
    "        self.log(\"val_loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in a pretrained model as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using <class 'sdofm.datasets.SDOML.SDOMLDataModule'> Data Class\n",
      "[* CACHE SYSTEM *] Found cached index data in /mnt/sdoml/cache/aligndata_AIA_FULL_12min.csv.\n",
      "[* CACHE SYSTEM *] Found cached normalization data in /mnt/sdoml/cache/normalizations_AIA_FULL_12min.json.\n",
      "[* CACHE SYSTEM *] Found cached HMI mask data in /mnt/sdoml/cache/hmi_mask_512x512.npy.\n",
      "Loading checkpoint...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from pretrain import Pretrainer\n",
    "\n",
    "MAE = Pretrainer(cfg, logger=None, is_backbone=True)\n",
    "backbone = MAE.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocalibration initialising\n",
      "input_channels: 32\n",
      "[32, 512, 512]\n",
      "cnn_output_dim: 401408\n"
     ]
    }
   ],
   "source": [
    "backbone_params = {}\n",
    "backbone_params[\"img_size\"] = cfg.model.mae.img_size\n",
    "backbone_params[\"patch_size\"] = cfg.model.mae.patch_size\n",
    "backbone_params[\"embed_dim\"] = cfg.model.mae.embed_dim\n",
    "backbone_params[\"num_frames\"] = cfg.model.mae.num_frames\n",
    "\n",
    "model = Autocalibration(\n",
    "    # backbone\n",
    "    **backbone_params,\n",
    "    # backbone\n",
    "    backbone=backbone,\n",
    "    hyperparam_ignore=[\"backbone\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name          | Type                                 | Params | Mode \n",
      "-------------------------------------------------------------------------------\n",
      "0 | backbone      | MAE                                  | 104 M  | eval \n",
      "1 | encoder       | WrapEncoder                          | 104 M  | eval \n",
      "2 | decoder       | ConvTransformerTokensToEmbeddingNeck | 78.1 K | train\n",
      "3 | head          | Autocalibration13Head                | 895 K  | train\n",
      "4 | loss_function | MSELoss                              | 0      | train\n",
      "-------------------------------------------------------------------------------\n",
      "973 K     Trainable params\n",
      "104 M     Non-trainable params\n",
      "105 M     Total params\n",
      "422.108   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f8a9bf02834667bab4acf4e2ca7b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 9])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b05e3693d104cfa8ac7b271251fcb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2414e6b57e19462fb2eef457687e8051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21caf837acd944e8b7a4003e3c4acd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "\n",
    "os.environ[\"PJRT_DEVICE\"] = \"GPU\"\n",
    "trainer = Trainer(max_epochs=2, precision=32)\n",
    "trainer.fit(model=model, datamodule=data_module)"
   ]
  },
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have questions, please join the conversation on [Hugging Face:](https://huggingface.co/SpaceML/SDO-FM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgements\n",
    "This work is the research product of the SDO-FM: A Multi-Modal Foundation Model POC for SDO (Grant#: 80NSSC24K0701); funded and supported by NASA. The research and its outputs have been designed, managed and delivered by Trillium Technologies Inc.\n",
    "\n",
    "**Authors**\n",
    "\n",
    "James Walsh, University of Cambridge  \n",
    "Daniel Gass, University of Central Lancashire  \n",
    "Raul Ramos Pollan, Universidad Industrial de Santander  \n",
    "Richard Galvez, Pure Storage  \n",
    "Paul Wright, Dublin Institute for Advanced Studies  \n",
    "Atılım Güneş Baydin, University of Oxford  \n",
    "Noah Kasmanoff, AE Studio   \n",
    "Jason Naradowsky, University of Tokyo  \n",
    "\n",
    "PI: Anne Spalding, Trillium Technolgies Inc  \n",
    "Co-I: James Parr, Trillium Technologies Inc "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
