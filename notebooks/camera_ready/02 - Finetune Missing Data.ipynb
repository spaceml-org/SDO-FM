{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data Reconstruction from Pretrained Embeddings\n",
    "\n",
    "For this example we're going to build on `01 - Finetune Virtual EVE.ipynb` and create a simpler finetuning set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import omegaconf\n",
    "from sdofm.datasets import SDOMLDataModule\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdofm.models import WrapEncoder, ConvTransformerTokensToEmbeddingNeck\n",
    "\n",
    "class MissingDataModel(BaseModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            # Backbone parameters\n",
    "            img_size: int = 512,\n",
    "            patch_size: int = 16,\n",
    "            embed_dim: int = 128,\n",
    "            num_frames: int = 1,\n",
    "            # Neck parameters\n",
    "            num_neck_filters: int = 32,\n",
    "            # Head parameters\n",
    "            # d_input=None,\n",
    "            cnn_model: str = \"efficientnet_b3\",\n",
    "            lr_linear: float = 0.01,\n",
    "            lr_cnn: float = 0.0001,\n",
    "            cnn_dp: float = 0.75,\n",
    "            epochs_linear: int = 50,\n",
    "            d_output=None,\n",
    "            eve_norm=None,\n",
    "            # for finetuning\n",
    "            backbone: object = None,\n",
    "            freeze_encoder: bool = True,\n",
    "            # all else\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        ):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.eve_norm = eve_norm\n",
    "\n",
    "            self.backbone = backbone\n",
    "\n",
    "            # WrapEncoder is a simple class to only run the forward\n",
    "            # encoder pass of the MAE. This allows us to easily\n",
    "            # freeze params as completed below.\n",
    "            self.encoder = WrapEncoder(self.backbone)\n",
    "\n",
    "            if freeze_encoder:\n",
    "                self.encoder.eval()\n",
    "                for param in self.encoder.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            num_tokens = img_size // patch_size\n",
    "\n",
    "            # As this is a reconstruction task, something that the MAE\n",
    "            # was designed to do, we don't require the neck.\n",
    "            \n",
    "            \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, eve = batch\n",
    "        x = self.encoder(imgs[:, :9, :, :])\n",
    "        y_hat = self.head(self.decoder(x))\n",
    "        loss = self.head.loss_func(y_hat, eve[:, :38, 0])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, eve = batch\n",
    "        x = self.encoder(imgs[:, :9, :, :])\n",
    "        y_hat = self.head(self.decoder(x))\n",
    "        # print(eve.shape)\n",
    "        loss = self.head.loss_func(y_hat, eve[:, :38, 0])\n",
    "        self.log(\"val_loss\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
