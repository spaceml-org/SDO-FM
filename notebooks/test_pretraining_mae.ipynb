{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The autoreload extension is already loaded. To reload it, use:\n",
                        "  %reload_ext autoreload\n"
                    ]
                }
            ],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "import pytorch_lightning as pl\n",
                "import torch\n",
                "import wandb\n",
                "from sdofm import utils\n",
                "from sdofm.datasets import SDOMLDataModule, DimmedSDOMLDataModule\n",
                "from sdofm.pretraining import MAE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import omegaconf\n",
                "\n",
                "cfg = omegaconf.OmegaConf.load(\"../experiments/pretrain_32.2M_mae_tpu.yaml\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[* CACHE SYSTEM *] Found cached index data in /mnt/sdoml/cache/aligndata_AIA_FULL_12min.csv.\n",
                        "[* CACHE SYSTEM *] Found cached normalization data in /mnt/sdoml/cache/normalizations_AIA_FULL_12min.json.\n",
                        "[* CACHE SYSTEM *] Found cached HMI mask data in /mnt/sdoml/cache/hmi_mask_512x512.npy.\n"
                    ]
                }
            ],
            "source": [
                "data_module = SDOMLDataModule(\n",
                "    hmi_path=None,\n",
                "    aia_path=os.path.join(\n",
                "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.aia\n",
                "    ),\n",
                "    eve_path=None,\n",
                "    components=cfg.data.sdoml.components,\n",
                "    wavelengths=cfg.data.sdoml.wavelengths,\n",
                "    ions=cfg.data.sdoml.ions,\n",
                "    frequency=cfg.data.sdoml.frequency,\n",
                "    batch_size=cfg.model.opt.batch_size,\n",
                "    num_workers=cfg.data.num_workers,\n",
                "    val_months=cfg.data.month_splits.val,\n",
                "    test_months=cfg.data.month_splits.test,\n",
                "    holdout_months=cfg.data.month_splits.holdout,\n",
                "    cache_dir=os.path.join(\n",
                "        cfg.data.sdoml.base_directory, cfg.data.sdoml.sub_directory.cache\n",
                "    ),\n",
                ")\n",
                "data_module.setup()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "'str' object is not callable",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMAE\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;43;03m#    **cfg.model.samae,\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;43;03m#    hmi_mask=data_module.hmi_mask,\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/home/walsh/repos/SDO-FM/sdofm/pretraining/MAE.py:37\u001b[0m, in \u001b[0;36mMAE.__init__\u001b[0;34m(self, img_size, patch_size, num_frames, tubelet_size, in_chans, embed_dim, depth, num_heads, decoder_embed_dim, decoder_depth, decoder_num_heads, mlp_ratio, norm_layer, norm_pix_loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# MAE specific\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     34\u001b[0m ):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mMaskedAutoencoderViT3D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtubelet_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_chans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_pix_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/home/walsh/repos/SDO-FM/sdofm/models/mae3d.py:100\u001b[0m, in \u001b[0;36mMaskedAutoencoderViT3D.__init__\u001b[0;34m(self, img_size, patch_size, num_frames, tubelet_size, in_chans, embed_dim, depth, num_heads, decoder_embed_dim, decoder_depth, decoder_num_heads, mlp_ratio, norm_layer, norm_pix_loss)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, embed_dim))\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(\n\u001b[1;32m     96\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, num_patches \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, embed_dim), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     97\u001b[0m )  \u001b[38;5;66;03m# fixed sin-cos embedding\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 100\u001b[0m     [\n\u001b[1;32m    101\u001b[0m         Block(\n\u001b[1;32m    102\u001b[0m             embed_dim,\n\u001b[1;32m    103\u001b[0m             num_heads,\n\u001b[1;32m    104\u001b[0m             mlp_ratio,\n\u001b[1;32m    105\u001b[0m             qkv_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    106\u001b[0m             norm_layer\u001b[38;5;241m=\u001b[39mnorm_layer,\n\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)\n\u001b[1;32m    109\u001b[0m     ]\n\u001b[1;32m    110\u001b[0m )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m norm_layer(embed_dim)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# MAE decoder specifics\u001b[39;00m\n",
                        "File \u001b[0;32m/home/walsh/repos/SDO-FM/sdofm/models/mae3d.py:101\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, embed_dim))\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(\n\u001b[1;32m     96\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, num_patches \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, embed_dim), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     97\u001b[0m )  \u001b[38;5;66;03m# fixed sin-cos embedding\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    100\u001b[0m     [\n\u001b[0;32m--> 101\u001b[0m         \u001b[43mBlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m            \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)\n\u001b[1;32m    109\u001b[0m     ]\n\u001b[1;32m    110\u001b[0m )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m norm_layer(embed_dim)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# MAE decoder specifics\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/envs/sdofm/lib/python3.10/site-packages/timm/models/vision_transformer.py:131\u001b[0m, in \u001b[0;36mBlock.__init__\u001b[0;34m(self, dim, num_heads, mlp_ratio, qkv_bias, qk_norm, proj_drop, attn_drop, init_values, drop_path, act_layer, norm_layer, mlp_layer)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    117\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m         mlp_layer\u001b[38;5;241m=\u001b[39mMlp,\n\u001b[1;32m    129\u001b[0m ):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1 \u001b[38;5;241m=\u001b[39m \u001b[43mnorm_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m Attention(\n\u001b[1;32m    133\u001b[0m         dim,\n\u001b[1;32m    134\u001b[0m         num_heads\u001b[38;5;241m=\u001b[39mnum_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m         norm_layer\u001b[38;5;241m=\u001b[39mnorm_layer,\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1 \u001b[38;5;241m=\u001b[39m LayerScale(dim, init_values\u001b[38;5;241m=\u001b[39minit_values) \u001b[38;5;28;01mif\u001b[39;00m init_values \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n",
                        "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
                    ]
                }
            ],
            "source": [
                "model = MAE(\n",
                "    **cfg.model.mae,\n",
                "#    **cfg.model.samae,\n",
                "#    hmi_mask=data_module.hmi_mask,\n",
                "    optimiser=cfg.model.opt.optimiser,\n",
                "    lr=cfg.model.opt.learning_rate,\n",
                "    weight_decay=cfg.model.opt.weight_decay,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer = pl.Trainer(\n",
                "    devices=1, accelerator=cfg.experiment.accelerator, max_epochs=cfg.model.opt.epochs\n",
                ")\n",
                "trainer.fit(model=model, datamodule=data_module)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rbg_image_batch = next(iter(data_module.train_dataloader()))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rbg_image_batch.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "single_image_batch = next(iter(data_module.train_dataloader()))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_module.aligndata.iloc[0].name"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "single_image_batch.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "patchified = model.autoencoder.patchify(\n",
                "    single_image_batch\n",
                ")  # pre single channel = torch.Size([1, 3072, 2304])\n",
                "patchified.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from einops import rearrange\n",
                "# p = patch_embed.patch_size[0]\n",
                "# tub = patch_embed.tubelet_size\n",
                "# x = rearrange(\n",
                "#     single_image_batch, \"b c (h p) (w q) -> b (h w) (4 p q c)\", p=p, q=p\n",
                "# )\n",
                "# x.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "patch_embed.tubelet_size"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.imshow(image_batch[0, 0, 0, :, :].cpu().numpy(), cmap=\"gray\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mask = data_module.hmi_mask\n",
                "print(mask.shape)\n",
                "plt.imshow(mask.cpu().numpy(), cmap=\"gray\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sdofm.utils import get_1d_sincos_pos_embed_from_grid, get_3d_sincos_pos_embed\n",
                "\n",
                "embed_dim = 128\n",
                "num_frames = 1\n",
                "tubelet_size = 1\n",
                "img_size = (512, 512)\n",
                "patch_size = (16, 16)\n",
                "grid_size = (\n",
                "    num_frames // tubelet_size,\n",
                "    img_size[0] // patch_size[0],\n",
                "    img_size[1] // patch_size[1],\n",
                ")\n",
                "num_patches = grid_size[0] * grid_size[1] * grid_size[2]\n",
                "pos_embed_zeros = torch.zeros(1, num_patches + 1, embed_dim)\n",
                "pos_embed = get_3d_sincos_pos_embed(\n",
                "    pos_embed_zeros.shape[-1], grid_size, cls_token=False\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "single_image_batch.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mask.shape\n",
                "a = np.repeat(mask[:, :, np.newaxis], 9, axis=2)\n",
                "b = np.repeat(a[:, :, :, np.newaxis], 1, axis=3)\n",
                "c = np.repeat(b[:, :, :, :, np.newaxis], 1, axis=4)\n",
                "# c.shape\n",
                "d = torch.Tensor(np.transpose(c, axes=(3, 2, 4, 0, 1))).to(dtype=torch.float)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mask.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "single_image_batch[0, 0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "patch_embed = PatchEmbed(512, 16, 1, 1, 9, 128, flatten=True)\n",
                "x = patch_embed(d)\n",
                "x.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "indices = np.arange(32 * 32).reshape(32, 32).astype(np.int64)\n",
                "ai = np.repeat(indices[:, :, np.newaxis], 128, axis=2)\n",
                "bi = np.repeat(ai[:, :, :, np.newaxis], 1, axis=3)\n",
                "ci = np.repeat(bi[:, :, :, :, np.newaxis], 1, axis=4)\n",
                "di = torch.Tensor(np.transpose(ci, axes=(3, 2, 4, 0, 1)))  # .to(dtype=torch.float)\n",
                "di.shape\n",
                "fi = di.flatten(2).transpose(1, 2)\n",
                "fi.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.scatter(x=range(1024), y=fi[0, :, 0].detach().numpy())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "15 * 15"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fi[0, :, 0].max()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.imshow(x[0, 0, 0, :, :].detach().numpy())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.imshow(x[0, :, :].detach().numpy(), aspect=\"auto\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pos_embed = get_3d_sincos_pos_embed(\n",
                "    pos_embed.shape[-1], patch_embed.grid_size, cls_token=False\n",
                ")\n",
                "pos_embed = torch.from_numpy(pos_embed).float().unsqueeze(0)\n",
                "pos_embed.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "patchified.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "patch_embed.grid_size[1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import astropy.units as u\n",
                "from astropy.coordinates import SkyCoord\n",
                "import sunpy.data.sample\n",
                "import sunpy.map\n",
                "from sunpy.coordinates.frames import HeliographicStonyhurst\n",
                "\n",
                "aiamap = sunpy.map.Map(\n",
                "    sunpy.data.sample.AIA_171_IMAGE\n",
                ")  # example image is loaded at 1024x1024\n",
                "\n",
                "\n",
                "def stonyhurst_to_patch_index(lat, lon):\n",
                "    # Heliographic Stonyhurst coordinates to patch index\n",
                "    # lat, lon = 15.73, 0\n",
                "    coord = SkyCoord(lat * u.deg, lon * u.deg, frame=HeliographicStonyhurst)\n",
                "    x, y = aiamap.wcs.world_to_pixel(coord)  # (x, y) in pixels\n",
                "    x, y = x / 2 // patch_embed.patch_size[0], y / 2 // patch_embed.patch_size[0]\n",
                "    return np.array([x, y])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "middle_patch = stonyhurst_to_patch_index(0, 0)\n",
                "middle_patch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "r1_patch = stonyhurst_to_patch_index(0, -60)[1]\n",
                "r2_patch = stonyhurst_to_patch_index(0, 60)[1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stonyhurst_to_patch_index(60, 0)\n",
                "# r2_patch = stonyhurst_to_patch_index(0, 60)[1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mean_patch = (\n",
                "    abs(\n",
                "        (stonyhurst_to_patch_index(15.73, 0) - stonyhurst_to_patch_index(0, 0))\n",
                "        + abs(stonyhurst_to_patch_index(-15.73, 0) - stonyhurst_to_patch_index(0, 0))\n",
                "    )\n",
                ")[0] / 2\n",
                "mean_patch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "std_patch = (stonyhurst_to_patch_index(6.14, 0) - stonyhurst_to_patch_index(0, 0))[0]\n",
                "std_patch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# torch.uniform(0, 32)\n",
                "# x = torch.distributions.uniform.Uniform(0,32).sample((1024,))\n",
                "# x\n",
                "\n",
                "# get uniform random numbers between [r1_patch, r2_patch]\n",
                "# (r1 - r2) * torch.rand(a, b) + r2\n",
                "# random_lons = torch.floor( (r1_patch - r2_patch ) * torch.rand((1024,)) + r2_patch ).to(dtype=torch.uint8)\n",
                "N = 2\n",
                "random_lons = torch.floor((r1_patch - r2_patch) * torch.rand((N, 1024)) + r2_patch).to(\n",
                "    dtype=torch.uint8\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "normal_lats = torch.floor(torch.normal(mean_patch, std_patch, size=(N, 1024)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "random_hemisphere = torch.floor(torch.rand((N, 1024)) * (2)).to(dtype=torch.int8)\n",
                "random_hemisphere[random_hemisphere == 0] = -1\n",
                "random_lats = random_hemisphere * normal_lats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "random_lons.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.scatter(random_lons[0, :], random_lats[0, :] + 15)\n",
                "plt.title(\"Per-hemisphere lat-normally lon-uniformly distributed patch locations\")\n",
                "plt.xlabel(\"Patch index (solar longitude)\")\n",
                "plt.ylabel(\"Patch index (solar latitude)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "N = 1\n",
                "L = 1024\n",
                "noise = torch.rand(N, L)  # noise in [0, 1]\n",
                "\n",
                "# sort noise for each sample\n",
                "ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
                "ids_restore = torch.argsort(ids_shuffle, dim=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ids_shuffle = random_lons * (random_lats + 15)\n",
                "ids_shuffle.to(dtype=torch.int16)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.argsort(ids_shuffle, dim=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ids_shuffle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "32 * 32"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "15 - 12\n",
                "19 - 15\n",
                "# patch_idx = x*y\n",
                "\n",
                "# Patch index to embedding index\n",
                "# frame_number"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.normal(3.5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "# Patch number to embedding index\n",
                "indices = np.arange(32 * 32).reshape(32, 32).astype(np.int64)\n",
                "ai = np.repeat(indices[:, :, np.newaxis], 128, axis=2)\n",
                "bi = np.repeat(ai[:, :, :, np.newaxis], 1, axis=3)\n",
                "ci = np.repeat(bi[:, :, :, :, np.newaxis], 1, axis=4)\n",
                "di = torch.Tensor(np.transpose(ci, axes=(3, 2, 4, 0, 1)))  # .to(dtype=torch.float)\n",
                "di.shape\n",
                "fi = di.flatten(2).transpose(1, 2)\n",
                "fi.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.where(fi[0, :, 0] == patch_idx)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "p = patch_embed.patch_size[0]\n",
                "num_p = patch_embed.img_size[0] // p\n",
                "tub = patch_embed.tubelet_size\n",
                "imgs = rearrange(\n",
                "    pos_embed,\n",
                "    \"b (t h w) (tub p q c) -> b c (t tub) (h p) (w q)\",\n",
                "    h=num_p,\n",
                "    w=num_p,\n",
                "    tub=tub,\n",
                "    p=p,\n",
                "    q=p,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_p"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.imshow(imgs[0, 0, 0, :, :].cpu().numpy(), cmap=\"gray\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "b = np.repeat(mask[:, :, np.newaxis], 4, axis=2)\n",
                "b = np.transpose(b, axes=[2, 0, 1])\n",
                "(b[0, :, :] == b[1, :, :])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sdofm.models.samae3d import PatchEmbed\n",
                "\n",
                "patch_embed = PatchEmbed(512, 16, 1, 1, 9, 128, flatten=False)\n",
                "\n",
                "nn.Conv3d(\n",
                "    in_chans=3,\n",
                "    embed_dim=128,\n",
                "    kernel_size=(1, 16, 16),\n",
                "    stride=(1, 16, 16),\n",
                "    bias=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "patch_embed(image_batch).shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "patch_embed.proj.weight.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "16 * 32"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The MaskedConv3D is a standard Conv3D with a binary mask applied on sampling locations that shouldn't contribute to the learning process. Whilst in theory a Conv3D could be written to take non-cubic input voxels this should achieve the same effect. The standard torch `nn.Conv3d` is modified such that \n",
                "\n",
                "In the simplest case, the output value of the layer with input size $(N, C_{in},D,H,W)$, output $(N, C_{out},D_{out},H_{out},W_{out})$, and logical mask $M$ can be described as:\n",
                "\n",
                "$$out(N_i, C_{out_j}) = bias(C_{out_j})+ \\sum_{C_{in}-1}^{k=0} M*weight(C_{out_j}, k) \\star input(N_i, k)$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "sdofm",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
